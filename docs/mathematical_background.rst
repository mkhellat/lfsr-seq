Mathematical Background
========================

This section provides a comprehensive mathematical treatment of Linear
Feedback Shift Registers (LFSRs), including theoretical foundations,
proofs, and detailed examples.

Notation and Conventions
-------------------------

This document uses the following unified notation system throughout:

**Finite Fields**:

* :math:`\mathbb{F}_q` denotes a finite field of order :math:`q = p^n`
  where :math:`p` is prime and :math:`n` is a positive integer
* :math:`\mathbb{F}_p` denotes a prime field (when :math:`n = 1`)
* :math:`\mathbb{F}_{p^n}` denotes an extension field (when :math:`n >
  1`)

**Scalar Sequence**:

* :math:`s_0, s_1, s_2, \ldots \in \mathbb{F}_q` denotes the infinite
  sequence generated by the LFSR

**State Vectors**:

* :math:`S_i = (s_i, s_{i+1}, \ldots, s_{i+d-1}) \in \mathbb{F}_q^d`
  denotes the state vector at time :math:`i`
* The state vector is a window of :math:`d` consecutive elements from
  the scalar sequence
* The relationship is: :math:`S_i[j] = s_{i+j}` for :math:`j = 0,
  \ldots, d-1`

**LFSR Parameters**:

* :math:`d` denotes the degree (length) of the LFSR
* :math:`c_0, c_1, \ldots, c_{d-1} \in \mathbb{F}_q` denote the
  feedback coefficients
* The coefficient vector is :math:`[c_0, c_1, \ldots, c_{d-1}]`

**Matrices**:

* :math:`C` denotes the state update matrix (companion matrix)
* :math:`I` denotes the identity matrix
* Matrix multiplication uses row vectors: :math:`S_{i+1} = S_i \cdot
  C`

**Polynomials**:

* :math:`P(t)` denotes the characteristic polynomial of the state
  update matrix
* :math:`f(x)` or :math:`f(t)` denotes general polynomials
  (context-dependent)

**Other Symbols**:

* :math:`\lambda` denotes the period of a sequence
* :math:`\mu` denotes the distance to cycle entry (in cycle detection)
* :math:`L(s)` denotes the linear complexity of sequence :math:`s`

Introduction to Linear Feedback Shift Registers
------------------------------------------------

A **Linear Feedback Shift Register (LFSR)** is a shift register whose
input is a linear function of its previous state. LFSRs are
fundamental building blocks in:

* **Cryptography**: Stream ciphers, key generation, pseudorandom
  number generation
* **Error Detection and Correction**: Cyclic redundancy checks (CRC),
  error-correcting codes
* **Signal Processing**: Scrambling, synchronization
* **Random Number Generation**: Pseudorandom sequences with known
  statistical properties

Mathematical Definition
~~~~~~~~~~~~~~~~~~~~~~~

An LFSR of degree :math:`d` over a finite field :math:`\mathbb{F}_q`
(where :math:`q = p^n` for prime :math:`p` and positive integer
:math:`n`) is defined by:

1. **Scalar sequence**: :math:`s_0, s_1, s_2, \ldots \in \mathbb{F}_q`
   (the infinite sequence generated by the LFSR)

2. **State vector**: :math:`S_i = (s_i, s_{i+1}, \ldots, s_{i+d-1})
   \in \mathbb{F}_q^d` (a window of :math:`d` consecutive elements
   from the scalar sequence)

3. **Feedback coefficients**: :math:`c_0, c_1, \ldots, c_{d-1} \in
   \mathbb{F}_q`

4. **Recurrence relation**: For :math:`i \geq 0`:

   .. math::

      s_{i+d} = c_0 s_i + c_1 s_{i+1} + \cdots + c_{d-1} s_{i+d-1} =
      \sum_{j=0}^{d-1} c_j s_{i+j}

The next state vector is computed by shifting all elements left and
computing the new rightmost element using the linear feedback
function:

.. math::

   S_{i+1} = (s_{i+1}, s_{i+2}, \ldots, s_{i+d-1}, s_{i+d})

Finite Fields (Galois Fields)
------------------------------

LFSRs operate over finite fields, also known as Galois fields, denoted
:math:`\mathbb{F}_q` (where :math:`q = p^n` for prime :math:`p` and
positive integer :math:`n`).

Prime Fields :math:`\mathbb{F}_p`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For prime :math:`p`, the field :math:`\mathbb{F}_p` consists of the
integers :math:`\{0, 1, 2, \ldots, p-1\}` with addition and
multiplication modulo :math:`p`.

.. prf:example:: Prime Field F_2
   :label: ex-prime-field-f2

   :math:`\mathbb{F}_2 = \{0, 1\}` with operations:

   .. math::

      \begin{aligned}
      0 + 0 &= 0, \quad 0 + 1 = 1, \quad 1 + 0 = 1, \quad 1 + 1 = 0 \\
      0 \cdot 0 &= 0, \quad 0 \cdot 1 = 0, \quad 1 \cdot 0 = 0, \quad 1 \cdot 1 = 1
      \end{aligned}

Extension Fields :math:`\mathbb{F}_{p^n}`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For :math:`n > 1`, :math:`\mathbb{F}_{p^n}` is an extension field of
:math:`\mathbb{F}_p` with :math:`p^n` elements. It can be constructed
as:

.. math::

   \mathbb{F}_{p^n} \cong \mathbb{F}_p[x] / \langle f(x) \rangle

where :math:`f(x)` is an irreducible polynomial of degree :math:`n`
over :math:`\mathbb{F}_p`.

.. prf:example:: Extension Field F_4
   :label: ex-extension-field-f4

   :math:`\mathbb{F}_4 = \mathbb{F}_2[x] / \langle x^2 + x +
   1 \rangle` has elements :math:`\{ 0, 1, \alpha, \alpha + 1 \}` where
   :math:`\alpha^2 + \alpha + 1 = 0`.

Field Properties
~~~~~~~~~~~~~~~~

* **Additive group**: :math:`(\mathbb{F}_q, +)` is an abelian group
* **Multiplicative group**: :math:`(\mathbb{F}_q^*, \cdot)` is a
  cyclic group of order :math:`q-1`
* **Primitive element**: There exists :math:`\alpha \in
  \mathbb{F}_q^*` such that :math:`\mathbb{F}_q^* = \{\alpha^0,
  \alpha^1, \ldots, \alpha^{q-2}\}`

Abstract Algebra Foundations
-----------------------------

To understand the deep connections between polynomials, matrices, and
scalars in LFSR analysis, we need fundamental concepts from abstract
algebra. This section provides formal definitions of rings, homomorphisms,
quotient rings, and related structures.

Rings
~~~~~

.. prf:definition:: Ring
   :label: def-ring

   A **ring** :math:`(R, +, \cdot)` is a set :math:`R` equipped with two
   binary operations, addition :math:`+` and multiplication :math:`\cdot`,
   such that:

   1. :math:`(R, +)` is an abelian group (additive group)
   2. Multiplication is associative: :math:`(a \cdot b) \cdot c = a \cdot (b \cdot c)`
   3. Multiplication distributes over addition:
      :math:`a \cdot (b + c) = a \cdot b + a \cdot c` and
      :math:`(a + b) \cdot c = a \cdot c + b \cdot c`

   If there exists an element :math:`1 \in R` such that :math:`1 \cdot a = a \cdot 1 = a`
   for all :math:`a \in R`, then :math:`R` is called a **ring with unity**
   (or **unital ring**).

**Examples**:

- The set of integers :math:`\mathbb{Z}` with usual addition and multiplication
- The polynomial ring :math:`\mathbb{F}[t]` over a field :math:`\mathbb{F}`
- The matrix ring :math:`M_d(\mathbb{F})` of :math:`d \times d` matrices over :math:`\mathbb{F}`
- Any field :math:`\mathbb{F}` (fields are special types of rings)

Ring Homomorphisms
~~~~~~~~~~~~~~~~~~

.. prf:definition:: Ring Homomorphism
   :label: def-ring-homomorphism

   Let :math:`R` and :math:`S` be rings. A function :math:`\phi: R \rightarrow S`
   is called a **ring homomorphism** if it preserves the ring operations:

   1. :math:`\phi(a + b) = \phi(a) + \phi(b)` for all :math:`a, b \in R` (additivity)
   2. :math:`\phi(a \cdot b) = \phi(a) \cdot \phi(b)` for all :math:`a, b \in R` (multiplicativity)
   3. If :math:`R` and :math:`S` are rings with unity, then :math:`\phi(1_R) = 1_S`

   The **kernel** of :math:`\phi` is the set:

   .. math::

      \ker(\phi) = \{a \in R : \phi(a) = 0_S\}

   where :math:`0_S` is the additive identity in :math:`S`.

**Key Properties**:

- The kernel :math:`\ker(\phi)` is an ideal of :math:`R`
- If :math:`\phi` is injective (one-to-one), then :math:`\ker(\phi) = \{0\}`
- If :math:`\phi` is surjective (onto), then :math:`\phi` is called an **epimorphism**
- If :math:`\phi` is bijective (one-to-one and onto), then :math:`\phi` is called an **isomorphism**

Ideals and Quotient Rings
~~~~~~~~~~~~~~~~~~~~~~~~~

.. prf:definition:: Ideal
   :label: def-ideal

   Let :math:`R` be a ring. A subset :math:`I \subseteq R` is called an
   **ideal** if:

   1. :math:`I` is a subgroup of :math:`(R, +)` (the additive group)
   2. For all :math:`r \in R` and :math:`i \in I`, we have :math:`r \cdot i \in I`
      and :math:`i \cdot r \in I` (absorption property)

   If :math:`I = \langle a \rangle = \{r \cdot a : r \in R\}` for some
   :math:`a \in R`, then :math:`I` is called a **principal ideal**
   generated by :math:`a`.

.. prf:definition:: Quotient Ring
   :label: def-quotient-ring

   Let :math:`R` be a ring and :math:`I` an ideal of :math:`R`. The
   **quotient ring** :math:`R/I` is the set of equivalence classes
   :math:`[r] = \{r + i : i \in I\}` for :math:`r \in R`, with operations:

   - :math:`[a] + [b] = [a + b]`
   - :math:`[a] \cdot [b] = [a \cdot b]`

   The elements :math:`[a]` and :math:`[b]` are considered equal (written
   :math:`a \equiv b \pmod{I}`) if :math:`a - b \in I`.

**Example**: For the polynomial ring :math:`\mathbb{F}[t]` and an ideal
:math:`I = \langle P(t) \rangle` generated by a polynomial :math:`P(t)`,
the quotient ring :math:`\mathbb{F}[t] / \langle P(t) \rangle` consists
of all polynomials modulo :math:`P(t)`. Two polynomials are equivalent if
their difference is divisible by :math:`P(t)`.

Isomorphisms
~~~~~~~~~~~~

.. prf:definition:: Ring Isomorphism
   :label: def-ring-isomorphism

   A **ring isomorphism** is a bijective ring homomorphism. If
   :math:`\phi: R \rightarrow S` is an isomorphism, we say that :math:`R`
   and :math:`S` are **isomorphic**, denoted :math:`R \cong S`.

   Isomorphic rings have identical algebraic structure—they differ only in
   the names of their elements. Any algebraic property true in one ring
   automatically holds in an isomorphic ring.

**First Isomorphism Theorem for Rings**:

If :math:`\phi: R \rightarrow S` is a ring homomorphism, then:

.. math::

   R / \ker(\phi) \cong \text{im}(\phi)

where :math:`\text{im}(\phi) = \{\phi(r) : r \in R\}` is the image of
:math:`\phi`.

This theorem is fundamental: it states that the quotient ring by the
kernel is isomorphic to the image of the homomorphism.

Evaluation Homomorphism
~~~~~~~~~~~~~~~~~~~~~~~

.. prf:definition:: Evaluation Homomorphism
   :label: def-evaluation-homomorphism

   Let :math:`\mathbb{F}[t]` be the polynomial ring over a field
   :math:`\mathbb{F}`, and let :math:`A` be an algebra over
   :math:`\mathbb{F}` (e.g., the matrix ring :math:`M_d(\mathbb{F})`).
   For a fixed element :math:`a \in A`, the **evaluation homomorphism**
   :math:`\phi_a: \mathbb{F}[t] \rightarrow A` is defined by:

   .. math::

      \phi_a(f(t)) = f(a)

   where :math:`f(a)` means substituting :math:`a` for the variable
   :math:`t` in the polynomial :math:`f(t)`.

**Properties**:

- :math:`\phi_a` is indeed a ring homomorphism
- The kernel :math:`\ker(\phi_a)` consists of all polynomials :math:`f(t)`
  such that :math:`f(a) = 0`
- If :math:`P(t)` is the minimal polynomial of :math:`a`, then
  :math:`\ker(\phi_a) = \langle P(t) \rangle`
- By the First Isomorphism Theorem:
  :math:`\mathbb{F}[t] / \ker(\phi_a) \cong \text{im}(\phi_a)`

**Application to LFSR**: For a companion matrix :math:`C`, the evaluation
homomorphism :math:`\phi_C: \mathbb{F}[t] \rightarrow M_d(\mathbb{F})`
maps polynomials to matrices. The Cayley-Hamilton theorem tells us that
the characteristic polynomial :math:`P(t)` lies in :math:`\ker(\phi_C)`,
establishing the connection between polynomial and matrix rings.

State Update Matrix and Companion Matrix
----------------------------------------

Matrix Representation
~~~~~~~~~~~~~~~~~~~~~~

The state transition of an LFSR can be represented as matrix
multiplication. Given state vector :math:`S_i = (s_i, s_{i+1}, \ldots,
s_{i+d-1})`, the next state is:

.. math::

   S_{i+1} = S_i \cdot C

where :math:`C` is the **state update matrix** (also called the
**companion matrix**), and :math:`S_i` is treated as a row vector.

Companion Matrix Construction
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For LFSR with coefficients :math:`c_0, c_1, \ldots, c_{d-1}`, the
companion matrix :math:`C` is a :math:`d \times d` matrix over
:math:`\mathbb{F}_q` with the following structure:

.. math::

   C = \begin{pmatrix}
   0 & 0 & 0 & \cdots & c_0 \\
   1 & 0 & 0 & \cdots & c_1 \\
   0 & 1 & 0 & \cdots & c_2 \\
   \vdots & \vdots & \ddots & \ddots & \vdots \\
   0 & 0 & \cdots & 1 & c_{d-1}
   \end{pmatrix}

**Structure**:

* **Subdiagonal**: For :math:`i = 1, 2, \ldots, d-1`, the entry at
  position :math:`(i, i-1)` is 1
* **Last column** (column :math:`d-1`): Contains the LFSR feedback coefficients
  :math:`c_0, c_1, \ldots, c_{d-1}` at positions :math:`(i, d-1)` for
  :math:`i = 0, \ldots, d-1`
* **All other entries**: 0

**Critical Note**: Coefficients are stored in the **last column**
(column :math:`d-1`), not the last row. This is essential for parallel
processing where coefficients must be extracted for matrix
reconstruction in worker processes using :math:`c_i = C[i, d-1]` for
:math:`i = 0, \ldots, d-1`.

**Proof of Correctness**:

Let :math:`S_i = (s_i, s_{i+1}, \ldots, s_{i+d-1})`. Then:

.. math::

   S_{i+1} = S_i \cdot C = (s_i, s_{i+1}, \ldots, s_{i+d-1}) \cdot C

Computing the product (row vector times matrix):

.. math::

   \begin{aligned}
   (S_i \cdot C)_0 &= s_i \cdot 0 + s_{i+1} \cdot 1 + s_{i+2} \cdot 0 + \cdots + s_{i+d-1} \cdot 0 = s_{i+1} \\
   (S_i \cdot C)_1 &= s_i \cdot 0 + s_{i+1} \cdot 0 + s_{i+2} \cdot 1 + \cdots + s_{i+d-1} \cdot 0 = s_{i+2} \\
   &\vdots \\
   (S_i \cdot C)_{d-2} &= s_{i+d-1} \\
   (S_i \cdot C)_{d-1} &= s_i \cdot c_0 + s_{i+1} \cdot c_1 + \cdots + s_{i+d-1} \cdot c_{d-1} = \sum_{j=0}^{d-1} c_j s_{i+j}
   \end{aligned}

By the recurrence relation:

.. math::

   s_{i+d} = \sum_{j=0}^{d-1} c_j s_{i+j}

Therefore:

.. math::

   S_{i+1} = (s_{i+1}, s_{i+2}, \ldots, s_{i+d-1}, s_{i+d})

The matrix :math:`C` correctly implements the shift operation (moving
elements left) and computes the new rightmost element using the linear
feedback function.

.. prf:example:: Companion Matrix Construction
   :label: ex-companion-matrix

   For LFSR with coefficients :math:`[1, 1, 0, 0]` over
   :math:`\mathbb{F}_2` (this gives the primitive polynomial :math:`t^4 + t + 1`):

   .. math::

      C = \begin{pmatrix}
      0 & 0 & 0 & 1 \\
      1 & 0 & 0 & 1 \\
      0 & 1 & 0 & 0 \\
      0 & 0 & 1 & 0
      \end{pmatrix}

   Given state :math:`S_0 = (s_0, s_1, s_2, s_3) = (1, 0, 0, 0)`:

   .. math::

      S_1 = S_0 \cdot C = (1, 0, 0, 0) \cdot C = (0, 0, 0, 1)

   where the last element is computed as :math:`s_4 = 1 \cdot 1 + 0 \cdot
   1 + 0 \cdot 0 + 0 \cdot 0 = 1`.

Characteristic Polynomial
-------------------------

Definition
~~~~~~~~~~

The **characteristic polynomial** of the state update matrix :math:`C`
is:

.. math::

   P(t) = \det(tI - C)

where :math:`I` is the :math:`d \times d` identity matrix.

Computation
~~~~~~~~~~~

For the companion matrix :math:`C`, the characteristic polynomial is:

.. math::

   P(t) = t^d - c_{d-1} t^{d-1} - c_{d-2} t^{d-2} - \cdots - c_1 t - c_0

**Note**: In fields where :math:`-1 \neq 1` (i.e., :math:`q > 2`), the
formula uses subtraction. In :math:`\mathbb{F}_2`, since :math:`-1 =
1`, the formula becomes:

.. math::

   P(t) = t^d + c_{d-1} t^{d-1} + c_{d-2} t^{d-2} + \cdots + c_1 t + c_0

.. prf:proof::

   Let :math:`C` be the companion matrix as defined above. Then:

   .. math::

      tI - C = \begin{pmatrix}
      t & 0 & 0 & \cdots & -c_0 \\
      -1 & t & 0 & \cdots & -c_1 \\
      0 & -1 & t & \cdots & -c_2 \\
      \vdots & \vdots & \ddots & \ddots & \vdots \\
      0 & 0 & \cdots & -1 & t - c_{d-1}
      \end{pmatrix}

   We compute :math:`\det(tI - C)` by expanding along the first column.
   The first column has entries :math:`t` at position :math:`(0,0)`,
   :math:`-1` at position :math:`(1,0)`, and zeros elsewhere. By the
   cofactor expansion formula:

   .. math::

      \det(tI - C) = t \cdot \det(M_0) - (-1) \cdot \det(M_1)

   where :math:`M_0` is the :math:`(d-1) \times (d-1)` submatrix obtained
   by deleting the first row and column, and :math:`M_1` is the
   :math:`(d-1) \times (d-1)` submatrix obtained by deleting the second
   row and first column.

   The submatrix :math:`M_0` has the same structure as :math:`tI - C` but
   of dimension :math:`d-1`. By induction, we obtain:

   .. math::

      \det(M_0) = t^{d-1} - c_{d-1} t^{d-2} - c_{d-2} t^{d-3} - \cdots - c_2 t - c_1

   The submatrix :math:`M_1` is lower triangular with :math:`-1` on the
   diagonal, hence :math:`\det(M_1) = (-1)^{d-1}`.

   Substituting into the expansion:

   .. math::

      \begin{aligned}
      \det(tI - C) &= t \cdot (t^{d-1} - c_{d-1} t^{d-2} - \cdots - c_2 t - c_1) - (-1) \cdot (-1)^{d-1} \\
                   &= t^d - c_{d-1} t^{d-1} - c_{d-2} t^{d-2} - \cdots - c_1 t - c_0
      \end{aligned}

   This establishes the desired result.

.. prf:example:: Characteristic Polynomial Computation
   :label: ex-char-poly-computation

   For :math:`C` with coefficients :math:`[1, 1, 0, 0]` over
   :math:`\mathbb{F}_2`:

   From the companion matrix:

   .. math::

      C = \begin{pmatrix}
      0 & 0 & 0 & 1 \\
      1 & 0 & 0 & 1 \\
      0 & 1 & 0 & 0 \\
      0 & 0 & 1 & 0
      \end{pmatrix}

   We compute:

   .. math::

      tI - C = \begin{pmatrix}
      t & 0 & 0 & -1 \\
      -1 & t & 0 & -1 \\
      0 & -1 & t & 0 \\
      0 & 0 & -1 & t
      \end{pmatrix}

   Using the general formula :math:`P(t) = t^d - c_{d-1} t^{d-1} -
   \cdots - c_0`:

   .. math::

      P(t) = t^4 - c_3 t^3 - c_2 t^2 - c_1 t - c_0 = t^4 - 0 \cdot t^3 -
      0 \cdot t^2 - 1 \cdot t - 1

   Since we are in :math:`\mathbb{F}_2` where :math:`-1 = 1`, this
   simplifies to:

   .. math::

      P(t) = t^4 + t + 1

   This polynomial is **primitive** and has order 15, giving maximum
   period.

Cayley-Hamilton Theorem
~~~~~~~~~~~~~~~~~~~~~~~

The characteristic polynomial :math:`P(t) = \det(tI - C)` encodes
fundamental information about the matrix :math:`C`. Remarkably, this
polynomial relationship extends beyond the scalar variable :math:`t` to
the matrix :math:`C` itself. The Cayley-Hamilton theorem establishes
this profound connection, revealing that matrices satisfy their own
characteristic equations.

**Theoretical Motivation: A Ring-Theoretic Perspective**

The Cayley-Hamilton theorem reveals a profound unification through the
lens of abstract algebra. Consider three mathematical structures:

1. **The polynomial ring** :math:`\mathbb{F}[t]` of polynomials in the
   indeterminate :math:`t` over the field :math:`\mathbb{F}`

2. **The matrix ring** :math:`M_d(\mathbb{F})` of :math:`d \times d`
   matrices over :math:`\mathbb{F}`

3. **The field** :math:`\mathbb{F}` itself (viewed as scalars)

The power of abstract algebra lies in recognizing that these seemingly
different objects can be treated uniformly within the framework of
**rings**. The Cayley-Hamilton theorem demonstrates that polynomials,
scalars, and matrices are not fundamentally different—they are different
representations of the same abstract algebraic structure.

**The Evaluation Homomorphism**

The key insight is the **evaluation map** :math:`\phi: \mathbb{F}[t]
\rightarrow M_d(\mathbb{F})` defined by :math:`\phi(f(t)) = f(C)`,
where :math:`f(C)` means substituting the matrix :math:`C` for the
variable :math:`t`. This map is a **ring homomorphism**, meaning it
preserves the ring operations:

- :math:`\phi(f + g) = \phi(f) + \phi(g)` (additivity)
- :math:`\phi(f \cdot g) = \phi(f) \cdot \phi(g)` (multiplicativity)
- :math:`\phi(1) = I` (preserves identity)

The characteristic polynomial :math:`P(t)` captures the eigenvalues of
:math:`C` as its roots in :math:`\mathbb{F}`. The Cayley-Hamilton
theorem asserts that this polynomial relationship extends to the matrix
ring: :math:`P(C) = 0` in :math:`M_d(\mathbb{F})`. This reveals that
:math:`P(t)` lies in the **kernel** of the evaluation homomorphism,
establishing a fundamental connection between the polynomial ring and
the matrix ring.

**Intuition**

Consider evaluating a polynomial :math:`f(t)` at a matrix :math:`C` by
substituting :math:`t = C`. The Cayley-Hamilton theorem states that
when we evaluate the characteristic polynomial :math:`P(t)` at
:math:`C`, we obtain the zero matrix. This means :math:`C` is a "root"
of its characteristic polynomial in the matrix sense, analogous to how
eigenvalues are roots in the scalar sense.

.. prf:theorem:: Cayley-Hamilton Theorem
   :label: theorem-cayley-hamilton

   Let :math:`C` be a :math:`d \times d` matrix over a field
   :math:`\mathbb{F}`, and let :math:`P(t) = \det(tI - C) = t^d - c_{d-1}
   t^{d-1} - \cdots - c_1 t - c_0` be its characteristic polynomial.
   Then :math:`C` satisfies its own characteristic equation:

   .. math::

      P(C) = C^d - c_{d-1} C^{d-1} - \cdots - c_1 C - c_0 I = 0

   where :math:`0` denotes the :math:`d \times d` zero matrix.

.. prf:proof::

   The proof relies on the adjugate matrix. Let :math:`\text{adj}(tI - C)`
   denote the adjugate of :math:`tI - C`. By the fundamental identity:

   .. math::

      (tI - C) \cdot \text{adj}(tI - C) = \det(tI - C) \cdot I = P(t) \cdot I

   The adjugate :math:`\text{adj}(tI - C)` is a matrix whose entries are
   polynomials in :math:`t` of degree at most :math:`d-1`. Therefore, we
   can write:

   .. math::

      \text{adj}(tI - C) = B_0 + B_1 t + B_2 t^2 + \cdots + B_{d-1} t^{d-1}

   for some :math:`d \times d` matrices :math:`B_0, B_1, \ldots, B_{d-1}`
   over :math:`\mathbb{F}`.

   Substituting into the identity and comparing coefficients of powers of
   :math:`t`, we obtain:

   .. math::

      P(t) I = (tI - C)(B_0 + B_1 t + \cdots + B_{d-1} t^{d-1})

   Expanding and equating coefficients yields a system of matrix equations.
   When we substitute :math:`t = C` into this polynomial identity, the
   left-hand side becomes :math:`P(C)`, while the right-hand side
   contains the factor :math:`(CI - C) = 0`, forcing :math:`P(C) = 0`.

   This establishes the theorem.

**Consequences and Applications**

The Cayley-Hamilton theorem has profound implications for matrix
analysis:

1. **Matrix Powers**: Any power :math:`C^k` with :math:`k \geq d` can be
   expressed as a linear combination of :math:`I, C, C^2, \ldots,
   C^{d-1}`. This follows by repeatedly applying the identity
   :math:`C^d = c_{d-1} C^{d-1} + \cdots + c_1 C + c_0 I`.

2. **Matrix Functions**: For any polynomial :math:`f(t)`, the evaluation
   :math:`f(C)` can be computed using polynomial division: write
   :math:`f(t) = q(t) P(t) + r(t)` where :math:`\deg(r) < d`, then
   :math:`f(C) = r(C)`.

3. **Minimal Polynomial**: The characteristic polynomial provides an
   upper bound on the degree of the minimal polynomial, which divides
   :math:`P(t)`.

4. **LFSR Analysis**: For the companion matrix :math:`C`, the theorem
   establishes that the state space has dimension at most :math:`d` in
   the sense that all matrix powers lie in the span of
   :math:`\{I, C, \ldots, C^{d-1}\}`. This is fundamental for computing
   the order of :math:`C` and understanding sequence periodicity.

5. **Computational Efficiency**: Instead of computing :math:`C^n` directly
   (which requires :math:`O(d^3 \log n)` operations), we can use the
   recurrence relation to compute it in :math:`O(d^2 \log n)` operations
   by expressing :math:`C^n` as a polynomial in :math:`C` of degree
   less than :math:`d`.

**Unification Through Abstract Algebra**

The ring-theoretic perspective reveals why the Cayley-Hamilton theorem
is so powerful: it demonstrates that **polynomials, scalars, and
matrices are different representations of the same abstract structure**.

Consider the following unified treatment:

- **Scalar evaluation**: For an eigenvalue :math:`\lambda \in \mathbb{F}`,
  we have :math:`P(\lambda) = 0` (by definition of eigenvalue)

- **Polynomial evaluation**: The polynomial :math:`P(t) \in \mathbb{F}[t]`
  encodes the algebraic relationship

- **Matrix evaluation**: The Cayley-Hamilton theorem states
  :math:`P(C) = 0` in :math:`M_d(\mathbb{F})`

All three statements express the same fundamental relationship, but in
different rings. The evaluation homomorphism :math:`\phi: \mathbb{F}[t]
\rightarrow M_d(\mathbb{F})` provides the bridge, showing that what is
true for scalars (eigenvalues) extends naturally to matrices through
the ring structure.

**The Power of Representation Independence**

This abstraction is the essence of linear algebra's power: we can work
with polynomials, compute with scalars, and reason about matrices, all
while operating within the same algebraic framework. The Cayley-Hamilton
theorem is not merely a computational tool—it is a manifestation of
the deep structural unity that abstract algebra reveals.

The fact that the same polynomial :math:`P(t)` can be evaluated at
scalars, treated as an abstract algebraic object, or evaluated at
matrices, demonstrates the **representation independence** that makes
abstract algebra so powerful. We are not working with three different
things—we are working with one thing (the polynomial) in three
different contexts (scalar field, polynomial ring, matrix ring).

**Connection to LFSR Periodicity**

For LFSR analysis, this ring-theoretic perspective provides the
theoretical foundation for understanding why sequence periods are bounded
and how they relate to the characteristic polynomial. The recurrence
relation :math:`C^d = c_{d-1} C^{d-1} + \cdots + c_1 C + c_0 I` directly
corresponds to the LFSR recurrence relation, establishing the deep
connection between the algebraic structure of the polynomial ring and
the dynamical behavior of the state sequence in the matrix ring.

The fact that :math:`P(C) = 0` means that in the quotient ring
:math:`M_d(\mathbb{F}) / \langle P(C) \rangle`, all powers of :math:`C`
reduce to linear combinations of :math:`I, C, \ldots, C^{d-1}`, which
directly explains the periodicity and boundedness of LFSR sequences.

Polynomial Order
----------------

Definition
~~~~~~~~~~

The **order** of a polynomial :math:`P(t)` over :math:`\mathbb{F}_q`
is the smallest positive integer :math:`n` such that:

.. math::

   t^n \equiv 1 \pmod{P(t)}

If no such :math:`n` exists (within the search space), the order is
infinite.

Connection to Matrix Order
~~~~~~~~~~~~~~~~~~~~~~~~~~

The relationship between polynomial order and matrix order exemplifies
the power of abstract algebra in unifying different mathematical
structures. Through the evaluation homomorphism :math:`\phi: \mathbb{F}[t]
\rightarrow M_d(\mathbb{F})`, we can translate questions about
polynomials into questions about matrices, and vice versa.

**Theoretical Framework**

Recall that the evaluation homomorphism :math:`\phi(f(t)) = f(C)` maps
polynomials to matrices. The Cayley-Hamilton theorem tells us that
:math:`P(t)` lies in the kernel of this homomorphism, i.e.,
:math:`\phi(P(t)) = P(C) = 0`. This means that in the quotient ring
:math:`\mathbb{F}[t] / \langle P(t) \rangle`, the polynomial :math:`t`
corresponds to the matrix :math:`C` under the induced isomorphism.

The order of :math:`P(t)` is the smallest :math:`n` such that
:math:`t^n \equiv 1 \pmod{P(t)}` in the quotient ring
:math:`\mathbb{F}[t] / \langle P(t) \rangle`. The order of :math:`C`
is the smallest :math:`n` such that :math:`C^n = I` in the matrix ring
:math:`M_d(\mathbb{F})`. The following theorem establishes their
equivalence.

.. prf:theorem:: Order Equivalence
   :label: theorem-order-equivalence

   Let :math:`C` be a companion matrix with characteristic polynomial
   :math:`P(t) = \det(tI - C)`. Then the order of :math:`P(t)` in the
   quotient ring :math:`\mathbb{F}[t] / \langle P(t) \rangle` equals the
   order of :math:`C` in the multiplicative semigroup of
   :math:`M_d(\mathbb{F})`.

.. prf:proof::

   Let :math:`n` be the order of :math:`C`, so :math:`C^n = I`. Consider
   the evaluation homomorphism :math:`\phi: \mathbb{F}[t] \rightarrow
   M_d(\mathbb{F})` defined by :math:`\phi(f(t)) = f(C)`.

   Since :math:`\phi(t^n) = C^n = I` and :math:`\phi(1) = I`, we have
   :math:`\phi(t^n - 1) = 0`. By the Cayley-Hamilton theorem,
   :math:`\phi(P(t)) = P(C) = 0`, so :math:`P(t)` lies in the kernel of
   :math:`\phi`.

   The fact that :math:`\phi(t^n - 1) = 0` means that :math:`t^n - 1` is
   also in the kernel. Since the kernel is an ideal containing
   :math:`P(t)`, and :math:`t^n - 1` evaluates to zero, we must have
   :math:`P(t) \mid (t^n - 1)` in :math:`\mathbb{F}[t]`, or equivalently,
   :math:`t^n \equiv 1 \pmod{P(t)}`.

   Since :math:`n` is the smallest positive integer with :math:`C^n = I`,
   and :math:`C^k \neq I` for any :math:`k < n`, we have
   :math:`\phi(t^k - 1) = C^k - I \neq 0` for :math:`k < n`. This implies
   :math:`t^k \not\equiv 1 \pmod{P(t)}` for :math:`k < n`. Therefore,
   :math:`n` is the smallest positive integer with :math:`t^n \equiv 1
   \pmod{P(t)}`, establishing that the order of :math:`P(t)` is :math:`n`.

   Conversely, suppose :math:`n` is the order of :math:`P(t)`, so
   :math:`t^n \equiv 1 \pmod{P(t)}`. Then there exists a polynomial
   :math:`q(t)` such that :math:`t^n - 1 = q(t) P(t)`. Applying the
   evaluation homomorphism:

   .. math::

      \phi(t^n - 1) = \phi(q(t)) \cdot \phi(P(t)) = \phi(q(t)) \cdot 0 = 0

   Therefore, :math:`C^n - I = 0`, so :math:`C^n = I`. If there existed
   :math:`k < n` with :math:`C^k = I`, then by the same argument we would
   have :math:`t^k \equiv 1 \pmod{P(t)}`, contradicting the minimality
   of :math:`n`. Hence, :math:`n` is the order of :math:`C`.

   This establishes the equivalence.

**Algebraic Interpretation**

This theorem reveals a deep structural connection: the multiplicative
order of :math:`t` in the quotient ring :math:`\mathbb{F}[t] / \langle
P(t) \rangle` (which is a field when :math:`P(t)` is irreducible)
corresponds exactly to the multiplicative order of :math:`C` in the
matrix ring. This is a manifestation of the fact that the evaluation
homomorphism induces an isomorphism between the quotient ring and the
subalgebra generated by :math:`C`.

**Computational Implications**

This equivalence provides powerful computational tools:

- We can compute matrix order by working in the polynomial ring, where
  we can use efficient polynomial arithmetic and modular reduction

- Conversely, we can verify polynomial order properties by computing
  matrix powers, which may be more efficient in certain contexts

- The connection enables us to leverage results from finite field theory
  (where polynomial orders are well-studied) to understand matrix
  behavior

.. prf:example:: Polynomial Order Computation
   :label: ex-poly-order

   For the primitive polynomial :math:`P(t) = t^4 + t + 1` over
   :math:`\mathbb{F}_2`:

   We check :math:`t^n \bmod P(t)` for increasing :math:`n`:

   * :math:`t^1 = t \not\equiv 1`
   * :math:`t^2 = t^2 \not\equiv 1`
   * :math:`t^3 = t^3 \not\equiv 1`
   * :math:`t^4 = t + 1 \not\equiv 1` (since :math:`t^4 = P(t) - (t + 1)
     = (t^4 + t + 1) - (t + 1)`)
   * :math:`t^5 = t \cdot t^4 = t(t + 1) = t^2 + t \not\equiv 1`
   * ... (continuing) ...
   * :math:`t^{15} \equiv 1 \pmod{P(t)}`

   So the order is 15, which equals :math:`2^4 - 1 = 15`, confirming that
   :math:`P(t)` is primitive.

Period and Sequence Analysis
-----------------------------

Matrix Order
~~~~~~~~~~~~

The **order** (or **period**) of matrix :math:`C` is the smallest
positive integer :math:`n` such that:

.. math::

   C^n = I

This represents the maximum period before any state sequence repeats.

**Properties**:

1. **Upper Bound**: The order :math:`n \leq q^d - 1` (by the
   pigeonhole principle)
2. **Divisibility**: The order divides :math:`q^d - 1` (by group
   theory)
3. **Maximal Period**: If :math:`P(t)` is primitive, then :math:`n =
   q^d - 1` (maximum possible)

State Sequence Periods
~~~~~~~~~~~~~~~~~~~~~~

For a given initial state :math:`S_0`, the sequence :math:`S_0, S_1,
S_2, \ldots` is periodic. The **period** of this sequence is the
smallest :math:`k` such that :math:`S_k = S_0`.

.. prf:theorem:: Period Divisibility
   :label: theorem-period-divisibility

   Let :math:`C` be the companion matrix of an LFSR with order :math:`n`
   (i.e., :math:`n` is the smallest positive integer such that
   :math:`C^n = I`). For any initial state :math:`S_0`, the period
   :math:`k` of the state sequence :math:`(S_0, S_1, S_2, \ldots)` divides
   :math:`n`.

.. prf:proof::

   Since :math:`C^n = I` by definition of the matrix order, we have for
   any state :math:`S_0`:

   .. math::

      S_n = S_0 \cdot C^n = S_0 \cdot I = S_0

   Therefore, the sequence returns to its initial state after :math:`n`
   steps, establishing that :math:`n` is a period (though not necessarily
   the minimal one).

   Now, let :math:`k` be the minimal period of the sequence, i.e., the
   smallest positive integer such that :math:`S_k = S_0`. We claim that
   :math:`k \mid n`.

   Since :math:`S_k = S_0`, the sequence is periodic with period :math:`k`,
   meaning :math:`S_{i+k} = S_i` for all :math:`i \geq 0`. In particular,
   :math:`S_{qk} = S_0` for any non-negative integer :math:`q`.

   Suppose, for contradiction, that :math:`k \nmid n`. By the division
   algorithm, there exist unique integers :math:`q \geq 0` and :math:`0 < r < k`
   such that:

   .. math::

      n = qk + r

   Since :math:`C^n = I`, we have :math:`S_n = S_0`. On the other hand, by
   the periodicity of the sequence:

   .. math::

      S_n = S_{qk + r} = S_r

   where the last equality follows from the fact that :math:`S_{qk} = S_0`
   and the sequence is periodic. Combining these, we obtain :math:`S_r = S_0`
   with :math:`0 < r < k`, which contradicts the minimality of :math:`k`.

   Therefore, we must have :math:`k \mid n`.

.. prf:example:: Period Divisibility
   :label: ex-period-divisibility

   For the 4-bit LFSR with primitive polynomial
   :math:`P(t) = t^4 + t + 1` over :math:`\mathbb{F}_2`:

   * Matrix order: 15
   * Possible sequence periods: 1, 3, 5, 15 (divisors of 15)
   * State :math:`(0,0,0,0)` has period 1 (all-zero state)
   * All other non-zero states have period 15 (maximum period)

Sequence Mapping Algorithm
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The algorithm for finding all sequences:

1. **Initialize**: Start with all :math:`q^d` possible state vectors
2. **Traverse**: For each unvisited state :math:`S_0`:
   
   a. Find the cycle period using cycle detection (see below)
   b. Generate the full sequence :math:`S_0, S_1, S_2, \ldots` using
      :math:`S_{i+1} = S_i \cdot C`
   c. Track visited states to avoid reprocessing
   d. Record period :math:`k` and sequence
3. **Categorize**: Group states by their sequence cycles

**Complexity Analysis**:

The algorithm's complexity is determined by the total number of possible
states in the LFSR state space.

* **Time Complexity**: :math:`O(q^d)`
  
  - **State Space Size**: There are exactly :math:`q^d` distinct state
    vectors (each of :math:`d` elements from :math:`\mathbb{F}_q`)
  
  - **State Visitation**: Each state is visited at most once during the
    traversal phase. When a state is encountered, it is either:
    
    * Already marked as visited (skipped in :math:`O(1)` time using a
      hash set or boolean array)
    
    * Unvisited, in which case we:
      
      - Detect the cycle period :math:`\lambda` using cycle detection
        (see below), requiring :math:`O(\lambda)` time
      
      - Generate the full sequence of length :math:`\lambda`, requiring
        :math:`O(\lambda)` matrix-vector multiplications
      
      - Mark all :math:`\lambda` states in the cycle as visited
  
  - **Total Operations**: Since each state belongs to exactly one cycle,
    and the sum of all cycle lengths equals :math:`q^d`, the total time
    is :math:`O(q^d)` state operations plus :math:`O(q^d)` matrix-vector
    multiplications. Assuming matrix-vector multiplication is :math:`O(d^2)`
    (which is constant for fixed :math:`d`), the overall time complexity
    is :math:`O(q^d)`.

* **Space Complexity**: :math:`O(q^d)`
  
  - **Visited State Tracking**: We maintain a data structure (hash set or
    boolean array) to track which of the :math:`q^d` states have been
    visited, requiring :math:`O(q^d)` space.
  
  - **Cycle Storage**: During cycle detection and sequence generation for
    a cycle of period :math:`\lambda`, we temporarily store up to
    :math:`\lambda` states. However, this is temporary space that can be
    reused across cycles. The maximum space needed at any point is
    :math:`O(\lambda_{\max})` where :math:`\lambda_{\max}` is the
    maximum cycle period, which is bounded by :math:`q^d - 1`.
  
  - **Output Storage**: If storing all sequences for output, additional
    space of :math:`O(q^d)` is needed (one state per sequence position).

* **Optimality**: This complexity is optimal in the worst case, as any
  algorithm that must examine all :math:`q^d` states requires at least
  :math:`\Omega(q^d)` time and space. The algorithm achieves this lower
  bound by visiting each state exactly once.

A critical component of the sequence mapping algorithm is the detection
of cycle periods for individual state sequences. The following section
examines the algorithmic approaches to this fundamental problem.

Cycle Detection Algorithms
~~~~~~~~~~~~~~~~~~~~~~~~~~

Determining the period of a state sequence :math:`(S_0, S_1, S_2, \ldots)`
generated by an LFSR is a fundamental computational problem. Given an
initial state :math:`S_0` and companion matrix :math:`C`, we seek the
smallest positive integer :math:`\lambda` such that :math:`S_\lambda = S_0`.

This section presents three algorithmic approaches to period detection:
naive enumeration, Floyd's cycle detection algorithm, and Brent's cycle
detection algorithm. We analyze their theoretical properties, computational
complexity, and practical performance characteristics, providing
comparative guidelines for algorithm selection.

Naive Enumeration Method
^^^^^^^^^^^^^^^^^^^^^^^^^

The **naive enumeration method** (also called **linear search**) is the
most straightforward approach to period detection.

**Algorithm Description**:

Given an initial state :math:`S_0` and companion matrix :math:`C`, the
algorithm iteratively computes states until a repetition is detected:

1. Initialize: Set :math:`S_{\text{current}} = S_0` and :math:`k = 0`
2. Iterate: While :math:`S_{\text{current}} \neq S_0` or :math:`k = 0`:
   
   a. Compute :math:`S_{\text{next}} = S_{\text{current}} \cdot C`
   b. Increment :math:`k \leftarrow k + 1`
   c. If :math:`S_{\text{next}} = S_0`, return period :math:`\lambda = k`
   d. Set :math:`S_{\text{current}} = S_{\text{next}}`

3. Return: The period :math:`\lambda = k`

**Mathematical Correctness**:

Since the state sequence is periodic (as established by the Period
Divisibility theorem), there exists a smallest positive integer
:math:`\lambda` such that :math:`S_\lambda = S_0`. The algorithm
systematically enumerates states :math:`S_0, S_1, S_2, \ldots, S_k` until
:math:`S_k = S_0`, at which point :math:`k = \lambda` by minimality.

**Complexity Analysis**:

* **Time Complexity**: :math:`O(\lambda)`
  
  The algorithm performs exactly :math:`\lambda` state transitions
  (matrix-vector multiplications) before detecting the period. Each
  transition requires :math:`O(d^2)` operations for matrix-vector
  multiplication, which is constant for fixed :math:`d`. Therefore, the
  total time is :math:`O(\lambda)`.

* **Space Complexity**: :math:`O(\lambda)` (full sequence mode) or
  :math:`O(1)` (period-only mode)
  
  - **Full Sequence Mode**: If the complete sequence must be stored for
    output, the algorithm requires :math:`O(\lambda)` space to store all
    :math:`\lambda` states in the cycle.
  
  - **Period-Only Mode**: If only the period value is needed, the
    algorithm can operate with :math:`O(1)` auxiliary space by storing
    only the current state and comparing it to :math:`S_0` at each step.
    However, this requires storing :math:`S_0` for comparison, which is
    :math:`O(d)` space (constant for fixed :math:`d`).

**Advantages**:

* **Simplicity**: Straightforward implementation with minimal code
  complexity
* **Optimal Time**: Achieves the theoretical lower bound of
  :math:`\Omega(\lambda)` operations
* **Cache Efficiency**: Sequential memory access pattern is
  cache-friendly
* **Practical Performance**: Typically 3-5× faster than Floyd's
  algorithm for small-to-medium periods

**Limitations**:

* **Space Requirement**: Full sequence mode requires storing all states
  in the cycle
* **No Early Termination**: Must enumerate the entire cycle even if
  period could be detected earlier

.. prf:example:: Naive Enumeration
   :label: ex-naive-enumeration

   Consider an LFSR with period :math:`\lambda = 15`. The algorithm
   proceeds as follows:

   * Step 0: :math:`S_0` (initial state)
   * Step 1: :math:`S_1 = S_0 \cdot C`
   * Step 2: :math:`S_2 = S_1 \cdot C`
   * ...
   * Step 15: :math:`S_{15} = S_{14} \cdot C = S_0`

   The algorithm detects that :math:`S_{15} = S_0` and returns the
   period :math:`\lambda = 15`. The algorithm performs exactly 15 state
   transitions before detecting the period.

Floyd's Cycle Detection Algorithm
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

**Floyd's cycle detection algorithm** (also known as the **tortoise and
hare algorithm**) is a two-pointer technique that can detect cycles in
sequences using only :math:`O(1)` auxiliary space for the detection
phase. Originally developed by Robert W. Floyd for cycle detection in
linked lists, it generalizes naturally to LFSR state sequences.

**Algorithm Description**:

The algorithm operates in two phases, using two pointers (tortoise and
hare) that traverse the sequence at different speeds.

**Phase 1 - Cycle Detection**:

1. Initialize both pointers at the initial state:
   :math:`T_0 = H_0 = S_0`
2. Iterate until the pointers meet:
   
   * Move tortoise one step: :math:`T_{i+1} = T_i \cdot C`
   * Move hare two steps: :math:`H_{i+1} = (H_i \cdot C) \cdot C`
   * Continue until :math:`T_j = H_j` for some :math:`j`

3. Let :math:`m` denote the meeting point index (i.e., :math:`T_m = H_m`)

**Phase 2 - Period Determination**:

1. Keep tortoise at meeting point: :math:`T = T_m = H_m = S_m`
2. Move hare one step from meeting point: :math:`H = S_m \cdot C`
3. Count iterations :math:`k` until hare returns to meeting point:
   
   * :math:`H_{k+1} = H_k \cdot C`
   * Continue until :math:`H_k = S_m`
   
4. The period is :math:`\lambda = k`

**Note on Phase 2 for LFSR Sequences**:

For LFSR sequences, which are purely periodic from :math:`S_0`, Phase 2 is
**unnecessary**. Phase 1 completes in exactly :math:`m = \lambda` iterations,
yielding :math:`\lambda = m` directly. The algorithm should terminate after
Phase 1 for LFSR sequences. Phase 2 is only required for general cycle
detection problems with non-cyclic tails (see below).

**Mathematical Correctness**:

We now provide a rigorous proof that Floyd's algorithm correctly
identifies the period :math:`\lambda` of the state sequence.

.. prf:theorem:: Floyd's Algorithm Correctness
   :label: theorem-floyd-correctness

   Floyd's cycle detection algorithm correctly identifies the period
   :math:`\lambda` of a periodic state sequence :math:`(S_0, S_1, S_2,
   \ldots)` generated by an LFSR.

.. prf:proof::

   Since the state sequence is periodic with period :math:`\lambda`, there
   exists a smallest positive integer :math:`\lambda` such that
   :math:`S_\lambda = S_0` and :math:`S_{i+\lambda} = S_i` for all
   :math:`i \geq 0`.

   **Phase 1 Analysis**:

   After :math:`i` iterations of Phase 1:
   
   * Tortoise position: :math:`T_i = S_i`
   * Hare position: :math:`H_i = S_{2i}`

   The pointers meet when :math:`T_i = H_i`, i.e., when :math:`S_i = S_{2i}`.

   Since the sequence is periodic with period :math:`\lambda`, we have:
   :math:`S_{2i} = S_{2i \bmod \lambda}`. For the pointers to meet, we
   require:

   .. math::

      S_i = S_{2i} = S_{2i \bmod \lambda}

   This implies :math:`i \equiv 2i \pmod{\lambda}`, which simplifies to
   :math:`i \equiv 0 \pmod{\lambda}`. Therefore, the pointers meet when
   :math:`i` is a multiple of :math:`\lambda`.

   Since :math:`i` must be a multiple of :math:`\lambda` for the meeting to
   occur, and the algorithm checks at each iteration, the meeting occurs at
   the smallest positive multiple, which is :math:`m = \lambda` (when
   :math:`k = 1`). Therefore, we have :math:`T_m = H_m = S_m = S_\lambda = S_0`.

   **Phase 2 Analysis**:

   In Phase 2, we keep the tortoise at the meeting point :math:`S_m = S_0`
   (since :math:`m = \lambda`) and move the hare one step at a time from
   :math:`S_m`. The hare will return to :math:`S_m = S_0` after exactly
   :math:`\lambda` iterations, since the sequence is periodic with period
   :math:`\lambda`.

   **LFSR Sequences**:

   For LFSR sequences, Phase 1 suffices: it completes in exactly
   :math:`m = \lambda` iterations, directly yielding the period. Phase 2 is
   unnecessary and should be omitted. The algorithm correctly identifies
   :math:`\lambda` as the period after Phase 1.

**When is Phase 2 Needed?**

Floyd's algorithm was designed for general cycle detection, where sequences
may have a **non-cyclic tail** before the cycle. For such problems, Phase
2 is essential.

**General Problem Structure**:

A sequence with tail has structure:

.. math::

   S_0 \to S_1 \to \cdots \to S_{\mu-1} \to S_\mu \to S_{\mu+1} \to \cdots \to S_{\mu+\lambda-1} \to S_\mu \to \cdots

where :math:`\mu \geq 0` is the tail length and :math:`\lambda` is the
cycle length. In Phase 1, the pointers meet at :math:`S_m` where
:math:`m = \mu + k\lambda` for some :math:`k \geq 1`, which is not
necessarily the cycle entry point :math:`S_\mu`.

**Example with Tail**:

Consider a linked list structure:

.. code-block:: none

   A → B → C → D → E → F → D → E → F → ...
           ↑               ↑
          tail          cycle entry
          (μ=3)         (cycle: D→E→F, λ=3)

Phase 1 execution:

* Iteration 0: T = A, H = A
* Iteration 1: T = B, H = C
* Iteration 2: T = C, H = E
* Iteration 3: T = D, H = D  ✓ MEET at D

The meeting occurs at D (which happens to be the cycle entry in this
example, but this is not guaranteed). However, we don't know:

1. **Where the cycle starts** (is D the entry point, or did we enter
   earlier?)
2. **The cycle length** :math:`\lambda` (we know we're in a cycle, but
   not its length)

**Phase 2 for General Problems**:

Phase 2 determines the cycle entry point :math:`S_\mu` (by resetting one
pointer to the start and moving both one step at a time until they meet)
and the cycle length :math:`\lambda` (by counting steps from the entry
point until returning to it).

**LFSR Sequences**:

LFSR sequences are purely periodic (:math:`\mu = 0`), so the cycle entry
point is :math:`S_0` and Phase 1 yields :math:`m = \lambda` directly.
Phase 2 is unnecessary.

**Complexity Analysis**:

* **Time Complexity**: :math:`O(\lambda)`
  
  The algorithm operates in two phases:
  
  - **Phase 1**: After :math:`m` iterations, the tortoise is at position
    :math:`S_m` and the hare is at position :math:`S_{2m}`. They meet when
    :math:`S_m = S_{2m}`, which occurs when :math:`m \equiv 2m
    \pmod{\lambda}`, i.e., when :math:`m \equiv 0 \pmod{\lambda}`. This
    means :math:`m` must be a multiple of :math:`\lambda`. Since the
    algorithm checks at each iteration and stops at the first meeting, the
    meeting occurs at the smallest positive multiple, which is :math:`m =
    \lambda`.
    
    **Visual Illustration** (period :math:`\lambda = 7`):
    
    Sequence: :math:`S_0, S_1, S_2, S_3, S_4, S_5, S_6, S_7 = S_0, S_8 = S_1, \ldots`
    
    .. code-block:: none
    
       Iteration 0:  T₀ = S₀,  H₀ = S₀          (both start at S₀)
       Iteration 1:  T₁ = S₁,  H₁ = S₂          (T at S₁, H at S₂)
       Iteration 2:  T₂ = S₂,  H₂ = S₄          (T at S₂, H at S₄)
       Iteration 3:  T₃ = S₃,  H₃ = S₆          (T at S₃, H at S₆)
       Iteration 4:  T₄ = S₄,  H₄ = S₁          (T at S₄, H at S₁)
       Iteration 5:  T₅ = S₅,  H₅ = S₃          (T at S₅, H at S₃)
       Iteration 6:  T₆ = S₆,  H₆ = S₅          (T at S₆, H at S₅)
       Iteration 7:  T₇ = S₇ = S₀,  H₇ = S₀     (T at S₀, H at S₀) ✓ MEET!
    
    Phase 1 completes after exactly :math:`m = 7 = \lambda` iterations.
  
  - **Phase 2** (for general problems): Requires :math:`\lambda` iterations
    and :math:`\lambda` state transitions to determine the cycle length.
    For LFSR sequences, Phase 2 is unnecessary as :math:`\lambda = m` is
    known from Phase 1.
  
  - **Overall for LFSR sequences**: The algorithm requires :math:`\lambda`
    iterations and :math:`3\lambda` state transitions (Phase 1 only),
    compared to :math:`\lambda` iterations and :math:`\lambda` state
    transitions for naive enumeration. The asymptotic complexity is
    :math:`O(\lambda)` in both cases.

* **Space Complexity**: :math:`O(1)` (period-only mode) or
  :math:`O(\lambda)` (full sequence mode)
  
  - **Period-Only Mode**: The algorithm maintains only two state pointers
    (tortoise and hare), each requiring :math:`O(d)` space to store a
    state vector. For fixed :math:`d`, this is constant space. The
    period-finding phase uses only :math:`O(1)` auxiliary space. Empirical
    measurements confirm constant memory usage (~1.60 KB) independent of
    period size.
  
  - **Full Sequence Mode**: If the complete sequence must be stored for
    output, the algorithm requires :math:`O(\lambda)` space to store all
    states, negating the space advantage of the period-finding phase.

**Comparison with Naive Enumeration**:

For LFSR sequences (where Phase 2 is omitted):

* **Iteration Count**: Both algorithms require :math:`\lambda` iterations.

* **State Transition Count**: Floyd performs :math:`3\lambda` transitions
  (tortoise: :math:`\lambda`, hare: :math:`2\lambda`) versus enumeration's
  :math:`\lambda` transitions, a **3× overhead** from the hare's
  double-speed traversal.

* **Time Performance**: For small-to-medium periods (< 1000),
  enumeration is typically **3-5× faster** in practice due to Floyd's
  higher state transition count, despite identical asymptotic
  :math:`O(\lambda)` complexity.

* **Space Performance**: Both achieve :math:`O(1)` auxiliary space in
  period-only mode. Enumeration uses slightly less memory (~1.44 KB vs
  ~1.60 KB) due to single-pointer state management.

* **Cache Performance**: Enumeration's sequential access pattern is more
  cache-friendly than Floyd's alternating pointer movements, which can
  cause cache misses when accessing distant states.

.. prf:example:: Floyd's Cycle Detection
   :label: ex-floyd-cycle

   Consider an LFSR with period :math:`\lambda = 15`. The state sequence
   is :math:`S_0, S_1, S_2, \ldots, S_{14}, S_{15} = S_0, \ldots`

   **Phase 1 - Cycle Detection**:

   Initialize both pointers at :math:`S_0`:

   * Iteration 0: :math:`T_0 = S_0`, :math:`H_0 = S_0`
   * Iteration 1: :math:`T_1 = S_1`, :math:`H_1 = S_2`
   * Iteration 2: :math:`T_2 = S_2`, :math:`H_2 = S_4`
   * Iteration 3: :math:`T_3 = S_3`, :math:`H_3 = S_6`
   * ...
   * Iteration 15: :math:`T_{15} = S_{15} = S_0`, :math:`H_{15} = S_{30} = S_0`

   The pointers meet at iteration 15. Since the meeting occurs when the
   iteration count is a multiple of :math:`\lambda`, and 15 is the smallest
   positive multiple of 15, we have :math:`m = 15 = \lambda`. Therefore,
   the period is :math:`\lambda = 15`, which we already know from Phase 1.

   **Phase 2 - Period Determination** (redundant for LFSR sequences):

   Keep tortoise at meeting point :math:`S_0`, move hare one step at a time:

   * Iteration 0: :math:`T = S_0`, :math:`H = S_1`
   * Iteration 1: :math:`T = S_0`, :math:`H = S_2`
   * ...
   * Iteration 14: :math:`T = S_0`, :math:`H = S_{14}`
   * Iteration 15: :math:`T = S_0`, :math:`H = S_{15} = S_0` ✓
   * The period is :math:`\lambda = 15`

   **Note**: Phase 2 is shown here for completeness, but it is redundant.
   Since Phase 1 already determined that the meeting occurred at iteration
   :math:`m = 15`, and :math:`m = \lambda` for LFSR sequences, we already
   know :math:`\lambda = 15` without needing Phase 2.

Brent's Cycle Detection Algorithm
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

**Brent's cycle detection algorithm** is an alternative two-pointer
technique developed by Richard P. Brent. Unlike Floyd's algorithm, which
uses pointers moving at different speeds, Brent's algorithm uses a
"power-of-two" strategy to detect cycles. Like Floyd's algorithm, it
achieves :math:`O(\lambda)` time complexity with :math:`O(1)` auxiliary
space in period-only mode.

**Algorithm Description**:

The algorithm maintains two pointers (tortoise and hare) and uses a
power variable that doubles at regular intervals:

1. **Initialize**: Set tortoise :math:`T = S_0`, hare :math:`H = S_0`,
   power :math:`p = 1`, and period counter :math:`\lambda = 0`

2. **Iterate**:
   
   a. Move hare one step: :math:`H \leftarrow H \cdot C`
   b. Increment period counter: :math:`\lambda \leftarrow \lambda + 1`
   c. If :math:`T = H`, return period :math:`\lambda`
   d. If :math:`\lambda = p`:
      
      * Reset tortoise to hare's position: :math:`T \leftarrow H`
      * Double the power: :math:`p \leftarrow 2p`
      * Reset period counter: :math:`\lambda \leftarrow 0`
   
   e. Repeat from step 2a

**Mathematical Correctness**:

The algorithm's correctness relies on the fact that when the period
counter reaches a power of 2, the tortoise is reset to the hare's
position. Since the power doubles each time (:math:`1, 2, 4, 8, 16,
\ldots`), the tortoise will eventually be positioned such that when the
hare completes a full cycle, they meet.

More formally, let :math:`p_k = 2^k` be the :math:`k`-th power value.
When :math:`\lambda = p_k`, the tortoise is set to the hare's current
position. The hare continues moving, and if the period is at most
:math:`p_k`, the tortoise and hare will meet within the next :math:`p_k`
steps. If the period is larger, the power doubles and the process
repeats with :math:`p_{k+1} = 2p_k`.

**Complexity Analysis**:

* **Time Complexity**: :math:`O(\lambda)`
  
  The algorithm performs at most :math:`\lambda + 2^{\lceil \log_2
  \lambda \rceil}` state transitions in the worst case. The power-of-two
  strategy ensures that the total number of operations is bounded by
  :math:`O(\lambda)`, though with a larger constant factor than naive
  enumeration.

* **Space Complexity**: :math:`O(1)` (period-only mode) or
  :math:`O(\lambda)` (full sequence mode)
  
  - **Period-Only Mode**: Similar to Floyd's algorithm, Brent's method
    maintains only two state pointers (tortoise and hare) and auxiliary
    variables (power, counter), achieving :math:`O(1)` space in
    period-only mode.
  
  - **Full Sequence Mode**: If the complete sequence must be stored for
    output, the algorithm requires :math:`O(\lambda)` space to store all
    states.

**Comparison with Floyd's Algorithm**:

* **Operation Pattern**: Brent's uses a power-of-two reset strategy,
  while Floyd's uses constant-speed differential traversal. Both
  achieve the same asymptotic complexity.

* **Operation Count**: Both algorithms perform approximately 3-4× more
  operations than naive enumeration due to their two-pointer
  approaches.

* **Practical Performance**: Similar to Floyd's, enumeration is
  typically 3-5× faster for small-to-medium periods due to simpler
  control flow and better cache locality.

* **Theoretical Interest**: Brent's algorithm provides an alternative
  approach to cycle detection, useful for verification and educational
  purposes.

.. prf:example:: Brent's Cycle Detection
   :label: ex-brent-cycle

   Consider an LFSR with period :math:`\lambda = 15`. The algorithm
   proceeds as follows:

   * Initialize: :math:`T = S_0`, :math:`H = S_0`, :math:`p = 1`,
     :math:`\lambda = 0`
   * Move hare: :math:`H = S_1`, :math:`\lambda = 1`
   * Since :math:`\lambda = p = 1`, reset: :math:`T = S_1`, :math:`p = 2`,
     :math:`\lambda = 0`
   * Continue: :math:`H = S_2`, :math:`\lambda = 1`; :math:`H = S_3`,
     :math:`\lambda = 2`
   * Since :math:`\lambda = p = 2`, reset: :math:`T = S_3`, :math:`p = 4`,
     :math:`\lambda = 0`
   * Continue until :math:`H` reaches a state where :math:`T = H`, at
     which point the period is determined

Algorithm Selection Guidelines
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Having examined three cycle detection algorithms (naive enumeration,
Floyd's, and Brent's), we now provide comparative guidelines for
algorithm selection based on theoretical properties and practical
considerations.

**Choose Naive Enumeration When**:

* Working with small-to-medium periods (< 1000)
* Simplicity and code maintainability are priorities
* Maximum performance is required for typical LFSR analysis
* Sequential memory access patterns are beneficial
* Default choice for most practical scenarios

**Choose Floyd's or Brent's Algorithm When**:

* Verifying results using an independent algorithmic approach
* Educational purposes to demonstrate alternative cycle detection
  techniques
* Very large periods where the constant-factor overhead might be
  amortized (requires empirical validation)
* Memory constraints are critical (though all achieve :math:`O(1)` space
  in period-only mode)
* Comparing different two-pointer techniques

**Comparative Summary**:

* **Time Complexity**: All three algorithms achieve :math:`O(\lambda)`
  asymptotic complexity, with enumeration having the smallest constant
  factor (~1×), Floyd's requiring ~4× operations, and Brent's requiring
  ~4× operations.

* **Space Complexity**: All achieve :math:`O(1)` space in period-only
  mode, with enumeration using slightly less memory (~1.44 KB vs ~1.60
  KB for Floyd's/Brent's).

* **Practical Performance**: Enumeration is typically 3-5× faster for
  small-to-medium periods due to simpler control flow and better cache
  locality.

* **Theoretical Interest**: Floyd's and Brent's provide alternative
  approaches useful for verification and educational purposes.

**Default Recommendation**:

For most practical LFSR analysis scenarios, naive enumeration is
recommended as the default choice due to its superior performance,
simplicity, and optimal time complexity with minimal constant factors.
Floyd's and Brent's algorithms serve as valuable alternatives for
verification and educational purposes.

Implementation Considerations
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

**Algorithm Selection**:

The implementation provides multiple cycle detection algorithms through
the ``--algorithm`` command-line option:

* **enumeration** (default): Naive enumeration method, recommended for
  most use cases
* **floyd**: Floyd's cycle detection algorithm
* **brent**: Brent's cycle detection algorithm (alternative two-pointer
  method)
* **auto**: Automatic selection based on heuristics

**Mode-Specific Behavior**:

* **Full Sequence Mode**: When generating complete sequences for output,
  all algorithms require :math:`O(\lambda)` space to store all states.
  Enumeration is the default choice due to superior performance.

* **Period-Only Mode** (``--period-only`` flag): When only the period
  value is needed, all algorithms achieve true :math:`O(1)` space
  complexity. Enumeration remains the default due to typically 3-5×
  better performance, with Floyd's and Brent's available as alternatives
  for verification or educational purposes.

**Performance Analysis**:

For detailed performance profiling and comparison of different cycle
detection algorithms, see the performance analysis script
(``scripts/performance_profile.py``) and the comprehensive performance
discussion in the user guide.

Parallel State Enumeration
---------------------------

For large LFSRs with state spaces containing thousands or millions of
states, sequential processing can be time-consuming. Parallel state
enumeration partitions the state space across multiple CPU cores to
achieve significant speedup on multi-core systems. The implementation
uses fork mode (13-17x faster than spawn) with SageMath isolation to
provide 2-4x speedup for large LFSRs.

**Motivation**:

The sequential algorithm processes states one at a time, which is
efficient for small LFSRs but becomes a bottleneck for large state
spaces. For an LFSR with :math:`q^d` states, sequential processing
requires :math:`O(q^d)` time. On a multi-core system with :math:`n`
cores, we can theoretically achieve up to :math:`n`-fold speedup by
processing states in parallel.

**Architecture**:

The parallel implementation provides two modes for work distribution:

* **Static Partitioning (Fixed Work Distribution)**:
  
  - **State Space Partitioning**: The entire state space is divided into
    :math:`n` roughly equal chunks, where :math:`n` is the number of worker
    processes. Each worker gets one fixed chunk.
  
  - **Independent Processing**: Each worker process processes its
    assigned chunk independently, finding cycles and computing periods
    without communication with other workers.
  
  - **Result Merging**: After all workers complete, results are merged
    with automatic deduplication of sequences that may have been found
    by multiple workers.
  
  - **Best For**: LFSRs with few cycles (2-4 cycles) or when cycles are
    evenly distributed.

* **Dynamic Load Balancing (Shared Task Queue)**:
  
  - **Task Queue Creation**: States are divided into small batches
    (typically 200 states) and placed in a shared queue accessible by
    all workers.
  
  - **Dynamic Work Distribution**: Workers continuously pull batches
    from the queue. When a worker finishes a batch, it immediately pulls
    the next available batch. Faster workers naturally take on more work,
    providing automatic load balancing and reducing imbalance by 2-4x for
    multi-cycle LFSRs.
  
  - **Result Merging**: After all workers complete, results are merged
    with automatic deduplication (same as static mode).
  
  - **Best For**: LFSRs with many cycles (8+ cycles), providing
    significantly better load balancing.

**Algorithm Description**:

The parallel algorithm operates in three main phases:

.. math::

   \begin{aligned}
   \text{Partition}(V, n) &: \text{Divide state space } V \text{ into } n \text{ chunks} \\
   \text{ProcessChunk}(C_i) &: \text{Process chunk } C_i \text{ independently} \\
   \text{Merge}(R_1, \ldots, R_n) &: \text{Combine and deduplicate results}
   \end{aligned}

**State Space Partitioning**:

The partitioning function divides the state space into chunks:

.. math::

   \text{chunk\_size} = \left\lceil \frac{|V|}{n} \right\rceil

   C_i = \{v_j : i \cdot \text{chunk\_size} \leq j < (i+1) \cdot \text{chunk\_size}\}

Each state is converted to a tuple (for pickling/serialization) since
SageMath vectors are not directly pickleable for inter-process communication.

**Implementation Details**:

* **Worker Processing**:
  
  Each worker process performs the following steps:
  
  1. Reconstructs SageMath objects from serialized data (tuples)
  2. Rebuilds the state update matrix from coefficients extracted from
     the **last column** of the companion matrix (not the last row). The
     companion matrix structure stores coefficients :math:`c_0, c_1,
     \ldots, c_{d-1}` in column :math:`d-1` at positions :math:`(i,
     d-1)` for :math:`i = 0, \ldots, d-1`. Extraction must use:
     :math:`c_i = C[i, d-1]` for :math:`i = 0, \ldots, d-1`. This is
     critical for correct matrix reconstruction in worker processes.
  3. Processes each state in its chunk:
     
     - Reconstructs state vector from tuple
     - **Period Computation**: Uses Floyd's algorithm
       (``_find_period_floyd``) to compute the period. Enumeration-based
       methods are avoided due to matrix multiplication loops that hang
       in multiprocessing context.
     - **Sequence Computation for Deduplication**: For periods
       :math:`\leq 100`, computes the full sequence using direct
       enumeration to enable proper deduplication. For larger periods,
       uses simplified deduplication based on
       :math:`(\text{start\_state}, \text{period})`.
     - Marks states in cycle as visited (local to worker)
     - Stores sequence information with ``period_only`` flag
  4. Returns results: sequences, periods, max period, errors

* **Algorithm Selection**:
  
  Floyd's algorithm is **required** for parallel processing because
  enumeration-based methods (which use matrix multiplication in tight
  loops) hang after approximately 12 iterations in multiprocessing
  context. This is a known SageMath/multiprocessing interaction issue.
  Only Floyd's algorithm is used in parallel workers, regardless of the
  ``--algorithm`` flag. Enumeration and Brent's algorithms are not used
  due to the matrix multiplication hang issue.

* **Period-Only Mode Requirement**:
  
  Parallel processing **requires** period-only mode (``--period-only``
  flag). Full sequence mode hangs due to the enumeration loop issue. The
  tool automatically forces period-only mode when parallel processing is
  enabled, displaying a warning to the user.

* **Result Merging and Deduplication**:

  Since multiple workers may process states from the same cycle, results
  must be deduplicated:
  
  - **For small periods** (:math:`\leq 100`): Workers compute the full
    sequence (even in period-only mode) for deduplication purposes. The
    merge function uses the sorted tuple of all state tuples in the
    cycle as the deduplication key. This ensures accurate deduplication
    since cycles are identical regardless of starting point.
  
  - **For large periods** (:math:`> 100`): To avoid hangs from matrix
    multiplication loops, workers use simplified deduplication based on
    :math:`(\text{start\_state}, \text{period})`. This may result in
    some false duplicates not being caught, but is an acceptable
    trade-off to avoid hangs.
  
  - **Period-Only Flag**: The merge function respects the
    ``period_only`` flag in sequence information. Even though full
    sequences may be computed for deduplication, they are not stored in
    the final output when ``period_only=True``.
  
  The merge function:
  
  1. Collects all sequences from all workers
  2. Creates canonical representations of cycles:
     - Small periods: Sorted tuple of all state tuples
     - Large periods: :math:`(\text{start\_state}, \text{period})` tuple
  3. Deduplicates based on cycle identity
  4. Reconstructs SageMath objects (only if ``period_only=False``)
  5. Assigns sequential sequence numbers
  6. Verifies correctness: :math:`\sum \text{periods} = q^d`

**Complexity Analysis**:

* **Time Complexity**: :math:`O(q^d / n)` per worker (theoretical),
  :math:`O(q^d)` total (amortized)
  
  Each worker processes approximately :math:`q^d / n` states
  independently. The total work remains :math:`O(q^d)`, but is
  distributed across :math:`n` workers.

* **Space Complexity**: :math:`O(q^d)` total (same as sequential,
  distributed across workers)
  
  Each worker maintains its own copy of SageMath objects, but total
  memory usage is similar to sequential processing since the state space
  is partitioned rather than duplicated.

* **Communication Overhead**: Minimal (only at start and end, no
  inter-worker communication)
  
  Workers operate independently with no communication during processing.
  Communication occurs only during initial partitioning and final result
  merging.

**Performance Characteristics**:

* **Theoretical Speedup**: Up to :math:`n`-fold on :math:`n` cores
  (linear scaling for independent work)
* **Practical Speedup**: 4-8× on typical multi-core systems (due to
  overhead from process creation, IPC, and result merging)
* **Best Case**: Large state spaces (> 10,000 states) with many CPU cores
* **Optimization Results**: After optimization (lazy partitioning),
  parallel processing achieves excellent speedup for medium-sized LFSRs:
  
  - **7-bit LFSR (128 states)**: 6.37× - 9.89× speedup
  - **Best configuration**: 1-2 workers for medium LFSRs
  - **Efficiency**: 159% - 989% (overhead reduction from optimization)

**Configuration**:

* **Automatic Selection**:
  
  The tool automatically enables parallel processing when:
  
  .. math::
  
     |V| > 10,000 \text{ and } n_{\text{cores}} \geq 2
  
  This threshold balances the overhead of multiprocessing against the
  benefits of parallelization.

* **Graceful Degradation**:
  
  If parallel processing fails or times out, the tool automatically
  falls back to sequential processing. This ensures:
  
  - **Reliability**: Tool always completes successfully
  - **Correctness**: Results are identical regardless of processing method
  - **User Experience**: No manual intervention required

**Known Limitations**:

* **Full Sequence Mode Hang**: Full sequence mode (without
  ``--period-only``) causes workers to hang during matrix
  multiplication loops in enumeration-based methods. This is a
  fundamental SageMath/multiprocessing interaction issue. **Workaround**:
  Parallel processing automatically forces period-only mode, displaying a
  warning. Use ``--no-parallel`` for full sequence mode.

* **Algorithm Restriction**: Only Floyd's algorithm is used in parallel
  workers, regardless of the ``--algorithm`` flag. Enumeration and
  Brent's algorithms are not used due to the matrix multiplication hang
  issue.

* **Deduplication for Large Periods**: For periods :math:`> 100`,
  deduplication uses simplified keys that may not catch all duplicates.
  This is an acceptable trade-off to avoid computing full sequences
  (which would hang).

* **SageMath Compatibility**: Some SageMath/multiprocessing
  configurations may cause workers to hang. The timeout mechanism
  detects this and falls back to sequential processing.

* **Small State Spaces**: Overhead of multiprocessing may outweigh
  benefits for small LFSRs (< 10,000 states). The automatic selection
  mechanism prevents parallel processing for small state spaces.

* **Matrix Coefficient Extraction**: Critical that coefficients are
  extracted from the **last column** of the companion matrix, not the
  last row. Incorrect extraction leads to wrong matrix reconstruction
  and incorrect period computations.

**Optimization Details**:

The main bottleneck (state space partitioning, 60% of time) was
optimized using lazy iteration:

* **Before**: Materialized all states upfront, then partitioned
* **After**: Lazy iteration with on-the-fly conversion to tuples
* **Result**: 6-10× speedup improvement for medium LFSRs

**Future Improvements**:

* Dynamic load balancing (instead of static partitioning)
* Shared memory for visited set (with proper locking)
* Progress tracking across workers
* Alternative parallelization approaches (threading, concurrent.futures)
* Further reduce process overhead (reuse workers, cache reconstruction)

Period Distribution Statistics
-------------------------------

The tool computes comprehensive statistical analysis of period
distribution across all sequences in an LFSR. This provides insights
into how periods are distributed and how they compare with theoretical
expectations.

**Distribution Metrics**:

The tool computes:

- **Mean Period**: Average period across all sequences
- **Median Period**: Middle value when periods are sorted
- **Variance**: Measure of how spread out periods are
- **Standard Deviation**: Square root of variance
- **Minimum/Maximum Period**: Smallest and largest periods
- **Period Frequency**: Histogram showing how many sequences have each
  period value

**Theoretical Bounds**:

For an LFSR of degree :math:`d` over :math:`\mathbb{F}_q`:

- **Maximum Theoretical Period**: :math:`q^d - 1` (all states except
  zero)
- **State Space Size**: :math:`q^d` (total number of possible states)

**Primitive Polynomial Analysis**:

When the characteristic polynomial is primitive:

- All non-zero states should have period :math:`q^d - 1`
- The period distribution should show all sequences with the maximum
  period
- This is verified automatically in the comparison section

**Period Diversity**:

The period diversity metric is defined as:

.. math::

   \text{Diversity} = \frac{\text{Unique Periods}}{\text{Total Sequences}}

A diversity of 1.0 means all sequences have different periods, while
lower values indicate more sequences share the same period.

**Comparison with Theoretical Bounds**:

The tool compares:

- Whether the maximum observed period equals the theoretical maximum
- The ratio of maximum period to theoretical maximum
- For primitive polynomials: whether all periods are maximum

This analysis helps validate theoretical predictions and understand
the structure of LFSR period distributions.

Polynomial Factorization and Factor Orders
------------------------------------------

Factorization
~~~~~~~~~~~~~

The characteristic polynomial can be factored over :math:`\mathbb{F}_q`:

.. math::

   P(t) = \prod_{i=1}^r f_i(t)^{e_i}

where :math:`f_i(t)` are irreducible polynomials and :math:`e_i` are
their multiplicities.

.. prf:example:: Polynomial Factorization
   :label: ex-poly-factorization

   Over :math:`\mathbb{F}_2`:

   .. math::

      t^4 + t^3 + t + 1 = (t+1)(t^3 + t + 1)

Factor Orders
~~~~~~~~~~~~~

Each factor :math:`f_i(t)` has its own order :math:`n_i` (smallest
:math:`n` such that :math:`t^n \equiv 1 \pmod{f_i(t)}`).

.. prf:theorem:: Order from Irreducible Factors
   :label: theorem-order-from-factors

   The order of :math:`P(t)` is the least common
   multiple (LCM) of the orders of its irreducible factors (with
   appropriate handling of multiplicities).

.. prf:proof::

   Proof sketch:

   If :math:`P(t) = f_1(t)^{e_1} f_2(t)^{e_2} \cdots f_r(t)^{e_r}`, and
   each :math:`f_i(t)` has order :math:`n_i`, then:

   * :math:`t^{n_i} \equiv 1 \pmod{f_i(t)}`
   * For :math:`t^n \equiv 1 \pmod{P(t)}`, we need :math:`t^n \equiv 1
     \pmod{f_i(t)^{e_i}}` for all :math:`i`
   * This requires :math:`n` to be a multiple of :math:`n_i` (and
     possibly :math:`p \cdot n_i` if :math:`e_i > 1` and :math:`p`
     divides :math:`n_i`)
   * Therefore, :math:`n = \text{lcm}(n_1, n_2, \ldots, n_r)` (with
     appropriate adjustments)

.. prf:example:: Order from Factors
   :label: ex-order-from-factors

   For :math:`P(t) = (t+1)(t^3 + t + 1)`:

   * Order of :math:`t+1`: 1 (since :math:`t \equiv -1 \equiv 1
     \pmod{t+1}` in :math:`\mathbb{F}_2`)
   * Order of :math:`t^3 + t + 1`: 7
   * Order of :math:`P(t)`: :math:`\text{lcm}(1, 7) = 7`

However, if the polynomial is not square-free, the calculation is more
complex.

Berlekamp-Massey Algorithm
---------------------------

Problem Statement
~~~~~~~~~~~~~~~~~

Given a sequence :math:`s_0, s_1, s_2, \ldots, s_{n-1}` over
:math:`\mathbb{F}_q`, find the **shortest** LFSR that can generate
this sequence.

Algorithm Description
~~~~~~~~~~~~~~~~~~~~~

The Berlekamp-Massey algorithm is an iterative algorithm that
constructs the minimal LFSR:

1. **Initialize**: Start with trivial LFSR of length 0
2. **Iterate**: For each new sequence element:
   
   a. Check if current LFSR correctly predicts the next element
   b. If correct, continue
   c. If incorrect (discrepancy found):

      * Update LFSR to correct the discrepancy
      * May need to increase LFSR length
3. **Output**: Minimal LFSR (coefficients and length)

Mathematical Foundation
~~~~~~~~~~~~~~~~~~~~~~~~

The algorithm maintains:

* **Current LFSR**: Represented by polynomial :math:`C(x) = 1 + c_1
  x + c_2 x^2 + \cdots + c_L x^L`
* **Discrepancy**: Difference between predicted and actual sequence
  value
* **Previous LFSR**: For backtracking when length increases

**Key Insight**: The minimal LFSR length equals the
  **linear complexity** of the sequence.

.. prf:theorem:: Berlekamp-Massey Correctness
   :label: theorem-berlekamp-massey

   The Berlekamp-Massey algorithm finds the unique minimal
   LFSR in :math:`O(n^2)` time.

.. prf:example:: Berlekamp-Massey Algorithm
   :label: ex-berlekamp-massey

   Sequence :math:`[1, 0, 1, 1, 0, 1, 0, 0, 1]` over
   :math:`\mathbb{F}_2`:

   * Initial: :math:`C(x) = 1`, length :math:`L = 0`
   * Process :math:`s_0 = 1`: No discrepancy, continue
   * Process :math:`s_1 = 0`: Discrepancy, update :math:`C(x) = 1 + x`, :math:`L = 1`
   * Process :math:`s_2 = 1`: Check prediction...
   * Continue until minimal LFSR found

   The algorithm will find that this sequence can be generated by an LFSR
   of length 4. The exact coefficients depend on the sequence and initial
   state.

Linear Complexity
-----------------

Definition
~~~~~~~~~~

The **linear complexity** :math:`L(s)` of a sequence :math:`s = s_0,
s_1, s_2, \ldots` is the length of the shortest LFSR that can generate
it.

Properties
~~~~~~~~~~

1. **Bounded**: For a sequence of length :math:`n`, :math:`0 \leq L(s)
   \leq n`
2. **Uniqueness**: The minimal LFSR is unique (up to initial state)
3. **Random Sequences**: A truly random sequence has linear complexity
   approximately :math:`n/2`

Linear Complexity Profile
~~~~~~~~~~~~~~~~~~~~~~~~~

The **linear complexity profile** is the sequence :math:`L_1, L_2,
\ldots, L_n` where :math:`L_i` is the linear complexity of the first
:math:`i` elements.

**Properties**:

* :math:`L_i \leq L_{i+1}` (complexity can only increase)
* :math:`L_{i+1} - L_i \leq 1` (complexity increases by at most 1 per
  step)
* If :math:`L_{i+1} > L_i`, then :math:`L_{i+1} = \max(L_i, i+1 - L_i)`

**Application**: Used in cryptanalysis to detect non-randomness in
 sequences.

Statistical Tests
-----------------

Frequency Test
~~~~~~~~~~~~~~

Tests whether the distribution of symbols in the sequence matches the
expected uniform distribution.

**Test Statistic**: For sequence of length :math:`n` over
 :math:`\mathbb{F}_q`:

.. math::

   \chi^2 = \sum_{a \in \mathbb{F}_q} \frac{(n_a - n/q)^2}{n/q}

where :math:`n_a` is the count of symbol :math:`a`.

**Expected**: :math:`\chi^2 \sim \chi^2(q-1)` under the null
 hypothesis of uniformity.

Runs Test
~~~~~~~~~

Tests for patterns and clustering in the sequence.

A **run** is a maximal subsequence of consecutive identical symbols.

**Test**: Count the number of runs and compare to expected
 distribution for a random sequence.

Autocorrelation
~~~~~~~~~~~~~~~

Measures the correlation between a sequence and shifted versions of
itself.

.. prf:definition:: Autocorrelation Function
   :label: def-autocorrelation

   For lag :math:`k`:

   .. math::

      R(k) = \frac{1}{n} \sum_{i=0}^{n-k-1} (-1)^{s_i + s_{i+k}}

   For binary sequences, this becomes:

   .. math::

      R(k) = \frac{1}{n} \sum_{i=0}^{n-k-1} (1 - 2(s_i \oplus s_{i+k}))

**Properties**:

* :math:`R(0) = 1` (perfect autocorrelation at lag 0)
* For random sequences, :math:`R(k) \approx 0` for :math:`k \neq 0`
* LFSR sequences have specific autocorrelation properties

Periodicity Test
~~~~~~~~~~~~~~~~

Detects periodic patterns in the sequence.

**Method**: Check if :math:`s_i = s_{i+k}` for various lags :math:`k`.

**Application**: Can reveal if sequence is periodic (which LFSR
 sequences are, by definition).

Comprehensive Example
---------------------

Let's work through a complete example: LFSR with coefficients
:math:`[1, 1, 0, 0]` over :math:`\mathbb{F}_2` (primitive polynomial
case).

Step 1: State Update Matrix
~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. math::

   C = \begin{pmatrix}
   0 & 0 & 0 & 1 \\
   1 & 0 & 0 & 1 \\
   0 & 1 & 0 & 0 \\
   0 & 0 & 1 & 0
   \end{pmatrix}

The coefficients :math:`c_0 = 1, c_1 = 1, c_2 = 0, c_3 = 0` are stored
in the last column at positions :math:`(0,3), (1,3), (2,3), (3,3)`
respectively.

Step 2: Characteristic Polynomial
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. math::

   P(t) = \det(tI - C) = t^4 + t + 1

This is a **primitive polynomial** over :math:`\mathbb{F}_2`.

Step 3: Polynomial Order
~~~~~~~~~~~~~~~~~~~~~~~~~

We verify that :math:`t^{15} \equiv 1 \pmod{P(t)}`:

* :math:`P(t) = t^4 + t + 1` (irreducible and primitive)
* Computing :math:`t^n \bmod P(t)` for :math:`n = 1, 2, \ldots, 15`
* Find :math:`t^{15} \equiv 1`, so order is 15
* Since :math:`15 = 2^4 - 1`, this confirms :math:`P(t)` is primitive

Step 4: Matrix Order
~~~~~~~~~~~~~~~~~~~~

Since the polynomial order equals the matrix order, :math:`C^{15} = I`.

Step 5: Sequence Analysis
~~~~~~~~~~~~~~~~~~~~~~~~~

Starting from state :math:`S_0 = (s_0, s_1, s_2, s_3) = (1, 0, 0, 0)`:

.. math::

   \begin{aligned}
   S_0 &= (1, 0, 0, 0) \\
   S_1 &= (0, 0, 0, 1) \\
   S_2 &= (0, 0, 1, 0) \\
   S_3 &= (0, 1, 0, 0) \\
   S_4 &= (1, 0, 0, 1) \\
   S_5 &= (0, 0, 1, 1) \\
   &\vdots \\
   S_{15} &= (1, 0, 0, 0) = S_0
   \end{aligned}

The sequence has period 15, which equals the matrix order. Since the
polynomial is primitive, all non-zero states have the maximum period
of 15.

Step 6: Factorization
~~~~~~~~~~~~~~~~~~~~~

Since :math:`P(t) = t^4 + t + 1` is **irreducible** (and primitive)
over :math:`\mathbb{F}_2`, it does not factor into lower-degree
polynomials.  The factorization is trivial: :math:`P(t) = t^4 + t + 1`
itself.

For a reducible polynomial example, consider :math:`Q(t) = t^4 + t^3 +
t + 1`:

.. math::

   Q(t) = t^4 + t^3 + t + 1 = (t+1)^2 (t^2 + t + 1)

* Factor :math:`(t+1)` has order 1
* Factor :math:`(t^2 + t + 1)` has order 3
* Order of :math:`Q(t)`: :math:`\text{lcm}(1, 3) = 3` (with
  appropriate handling of the square factor)

**Note**: The order calculation for polynomials with repeated factors
requires considering the multiplicities. For square-free polynomials,
the order is the LCM of the orders of the irreducible factors.

Theoretical Results and Theorems
---------------------------------

.. prf:theorem:: Maximum Period
   :label: theorem-maximum-period

   For an LFSR of degree :math:`d` over
   :math:`\mathbb{F}_q`, the maximum possible period is :math:`q^d - 1`.

.. prf:proof::

   * There are :math:`q^d` possible states
   * The all-zero state :math:`(0, 0, \ldots, 0)` is fixed (period 1)
   * All other states form cycles
   * Maximum cycle length is :math:`q^d - 1`

**Achievability**: The maximum period is achieved if and only if the
 characteristic polynomial is **primitive**.

.. prf:definition:: Primitive Polynomial
   :label: def-primitive-polynomial

   A polynomial :math:`P(t)` of degree :math:`d` over
   :math:`\mathbb{F}_q` is **primitive** if:

   1. :math:`P(t)` is irreducible
   2. The order of :math:`P(t)` is :math:`q^d - 1`

.. prf:theorem:: Primitive Polynomial Period
   :label: theorem-primitive-period

   If :math:`P(t)` is primitive, then the LFSR has maximum
   period :math:`q^d - 1`, and the generated sequence has excellent
   statistical properties.

.. prf:example:: Primitive Polynomial Example
   :label: ex-primitive-polynomial

   Over :math:`\mathbb{F}_2`, the polynomial :math:`t^4 +
   t + 1` is primitive and has order 15, giving maximum period.

**Implementation**: The tool automatically detects primitive
 polynomials and displays a ``[PRIMITIVE]`` indicator in the
 characteristic polynomial output. This can be explicitly checked
 using the ``--check-primitive`` command-line flag. The detection uses
 SageMath's built-in ``is_primitive()`` method when available, or
 falls back to checking irreducibility and verifying that the
 polynomial order equals :math:`q^d - 1`.

.. prf:theorem:: All Periods Divide Matrix Order
   :label: theorem-all-periods-divide

   All sequence periods divide the matrix order.

.. prf:proof::

   If :math:`C^n = I` and sequence has period :math:`k`, then:

   .. math::

      S_k = S_0 \Rightarrow S_0 \cdot C^k = S_0

   Since :math:`C^n = I`, we have :math:`S_0 = S_0 \cdot C^n`. If
   :math:`k` doesn't divide :math:`n`, we can write :math:`n = qk + r`
   with :math:`0 < r < k`, leading to a contradiction.

Therefore, :math:`k \mid n`.

.. prf:theorem:: Linear Complexity Lower Bound
   :label: theorem-linear-complexity-lower-bound

   For a sequence of length :math:`n` over
   :math:`\mathbb{F}_q`, the linear complexity :math:`L` satisfies:

.. math::

   L \geq \frac{n}{2}

with high probability for random sequences.

**Implication**: Sequences with low linear complexity are
 cryptographically weak.

Applications in Cryptography
----------------------------

Stream Ciphers
~~~~~~~~~~~~~~

LFSRs are used in stream ciphers (e.g., A5/1, A5/2 in GSM):

* **Advantages**: Fast, simple hardware implementation
* **Disadvantages**: Linear structure makes them vulnerable to attacks
* **Solution**: Combine multiple LFSRs with nonlinear functions

Key Generation
~~~~~~~~~~~~~~

LFSRs can generate pseudorandom sequences for key material, but
require:

* Primitive polynomials for maximum period
* Nonlinear combination for security
* Proper initialization (avoid all-zero state)

Cryptanalysis
~~~~~~~~~~~~~

Attacks on LFSR-based systems:

* **Berlekamp-Massey Attack**: Recover LFSR from known plaintext
* **Correlation Attack**: Exploit correlations in combined LFSRs
* **Fast Correlation Attack**: Use iterative decoding for efficient
  state recovery
* **Distinguishing Attack**: Detect if keystream is distinguishable
  from random
* **Algebraic Attack**: Solve systems of equations
* **Time-Memory Trade-Off (TMTO)**: Precompute states for faster
  attacks

.. _performance-analysis:

Performance Analysis and Algorithm Comparison
-----------------------------------------------

This section provides detailed analysis of cycle detection algorithm
performance based on empirical testing and theoretical analysis.

Operation Count Analysis
~~~~~~~~~~~~~~~~~~~~~~~~~

**Floyd's Algorithm Operation Count**:

For a period :math:`\lambda`, Floyd's algorithm performs:

* **Phase 1** (Find Meeting Point):
  
  * Tortoise moves: :math:`\sim \lambda/2` steps (on average for LFSRs)
  * Hare moves: :math:`2 \times \lambda/2 = \lambda` steps (double speed)
  * Total Phase 1 operations: :math:`3 \times \lambda/2 = 1.5\lambda`
    matrix multiplications

* **Phase 2** (Find Period):
  
  * Hare moves: :math:`\lambda` steps
  * Total Phase 2 operations: :math:`\lambda` matrix multiplications

* **Total Floyd Operations**: :math:`\sim 1.5\lambda + \lambda =
  2.5\lambda` operations

**Enumeration Algorithm Operation Count**:

* **Total Operations**: :math:`\lambda` matrix multiplications (one
  per state in cycle)

**Comparison**:

* Floyd performs approximately **2.5× more operations** than
  enumeration
* Actual measured ratio: **~3.83×** (due to implementation details and
  meeting point location)
* For period 24: Floyd = 92 operations, Enumeration = 24 operations

Time Performance Analysis
~~~~~~~~~~~~~~~~~~~~~~~~~

**Empirical Results** (Period-Only Mode):

For typical LFSR periods (8-24):

* **Floyd**: ~2.0 ms (92 operations for period 24)
* **Enumeration**: ~0.5 ms (24 operations for period 24)
* **Speedup**: Enumeration is **3-5× faster**
* **Time per Operation**: Similar (~0.022 ms for both algorithms)

**Why Floyd is Slower**:

1. **More Operations**: Floyd does ~4× more matrix multiplications
2. **Overhead Dominates**: Phase 1 + Phase 2 overhead outweighs
   benefits for small periods
3. **No Compensating Advantage**: Both algorithms are O(1) space, so
   Floyd's theoretical advantage doesn't apply

**Scaling Behavior**:

* For periods < 100: Enumeration is consistently faster
* For periods 100-1000: Enumeration remains faster (overhead still
  dominates)
* For periods > 1000: Needs testing, but overhead likely still
  dominates for typical LFSRs

Space Complexity Analysis
~~~~~~~~~~~~~~~~~~~~~~~~~

**Period-Only Mode** (``--period-only``):

Both algorithms achieve **true O(1) space**:

* **Floyd**: ~1.60 KB (constant, verified across iterations and period
  sizes)
* **Enumeration**: ~1.44 KB (constant, verified across iterations and
  period sizes)
* **Memory Independence**: Memory usage is constant regardless of
  period size ✓

**Full Sequence Mode**:

Both algorithms use **O(λ) space**:

* Must store full sequence for output
* Floyd's O(1) space advantage doesn't apply
* Enumeration is simpler and faster

**Verification**:

Memory profiling shows:

* Coefficient of variation < 0.1% for both algorithms in period-only
  mode
* Memory usage constant across period range 8-24
* True O(1) space confirmed ✓

Algorithm Selection Guidelines
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Use Enumeration When**:

* Computing full sequences (default, faster)
* Computing periods only (default, faster)
* Period < 1000 (typical case)
* Simplicity and speed are priorities

**Use Floyd When**:

* Educational/verification purposes
* Very large periods (> 10,000) - needs verification
* Want to verify results with different algorithm
* Exploring algorithm properties

**Default Behavior**:

* **Full Mode**: Enumeration (faster, simpler)
* **Period-Only Mode**: Enumeration (faster, both are O(1) space)
* **Auto Mode**: Selects enumeration for full mode, Floyd for
  period-only (but enumeration is still recommended)

Performance Profiling Tools
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The tool provides several profiling scripts:

* ``scripts/performance_profile.py``: Comprehensive algorithm
  comparison
  
  * Use ``--period-only`` flag for period-only mode analysis
  * Measures time, memory, and operation counts
  * Verifies space complexity claims

* ``scripts/detailed_performance_analysis.py``: Phase-by-phase
  analysis
  
  * Breaks down Floyd into Phase 1 and Phase 2
  * Analyzes operation counts in detail
  * Tests memory patterns

* ``scripts/analyze_floyd_overhead.py``: Overhead analysis
  
  * Explains why Floyd does more operations
  * Compares with different period sizes
  * Finds break-even points

**Example Usage**:

.. code-block:: bash

   # Compare algorithms in period-only mode
   python3 scripts/performance_profile.py input.csv 2 --period-only -n 10
   
   # Detailed phase analysis
   python3 scripts/detailed_performance_analysis.py input.csv 2
   
   # Overhead analysis
   python3 scripts/analyze_floyd_overhead.py input.csv 2

Summary and Recommendations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Key Findings**:

1. ✅ **Floyd is correctly implemented** - algorithm works as designed
2. ✅ **O(1) space achieved** - both algorithms achieve true O(1)
   space in period-only mode
3. ❌ **Floyd is slower** - does ~4× more operations, making it 3-5×
   slower
4. ❌ **No practical advantage** - enumeration is better for typical
   LFSR periods

**Recommendations**:

1. **Default to Enumeration**: Simpler, faster, uses less memory
2. **Keep Floyd as Option**: For educational and verification purposes
3. **Document Trade-offs**: Clearly explain performance
   characteristics
4. **Use Period-Only Mode**: When only periods are needed, both
   achieve O(1) space

**Conclusion**:

Floyd's cycle detection algorithm is a classic algorithm with
theoretical elegance, but for practical LFSR analysis, enumeration is
the better choice. Floyd's O(1) space advantage is achieved by
enumeration in period-only mode, and enumeration's simplicity and
speed make it superior for typical use cases.

References and Further Reading
-------------------------------

Classical Texts
~~~~~~~~~~~~~~~

* **Golomb, S. W.** (1967). *Shift Register
  Sequences*. Holden-Day. The foundational text on LFSRs and
  pseudorandom sequences.

* **Lidl, R. & Niederreiter, H.** (1997). *Finite Fields*. Cambridge
  University Press. Comprehensive treatment of finite field theory.

Modern References
~~~~~~~~~~~~~~~~~

* **Menezes, A. J., van Oorschot, P. C., & Vanstone, S. A.**
  (1996). *Handbook of Applied Cryptography*. CRC Press. Chapter 6
  covers LFSRs and stream ciphers.

* **Rueppel, R. A.** (1986). *Analysis and Design of Stream
  Ciphers*. Springer. Detailed analysis of LFSR-based cryptographic
  systems.

Online Resources
~~~~~~~~~~~~~~~~

* **Tanja Lange's Cryptology Course**:
  https://www.hyperelliptic.org/tanja/teaching/CS22/
  
   * Exercise 2 motivates this tool's development
   * Excellent introduction to LFSRs and their analysis

* **SageMath Documentation**: https://doc.sagemath.org/
  
   * Finite field operations
   * Polynomial manipulation
   * Matrix computations over finite fields

Mathematical Software
~~~~~~~~~~~~~~~~~~~~~

* **SageMath**: Open-source mathematics software system used by this
  tool
* **Magma**: Commercial computer algebra system with excellent finite
  field support
* **GAP**: System for computational discrete algebra

---

**Note**: This mathematical background provides the theoretical
 foundation for understanding LFSR analysis. For practical usage
 examples, see the :doc:`examples` section. For API documentation, see
 the :doc:`api/index` section.
