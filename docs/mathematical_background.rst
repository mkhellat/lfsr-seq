Mathematical Background
========================

This section provides a comprehensive mathematical treatment of Linear
Feedback Shift Registers (LFSRs), including theoretical foundations,
proofs, and detailed examples.

Notation and Conventions
-------------------------

This document uses the following unified notation system throughout:

**Finite Fields**:

* :math:`\mathbb{F}_q` denotes a finite field of order :math:`q = p^n`
  where :math:`p` is prime and :math:`n` is a positive integer
* :math:`\mathbb{F}_p` denotes a prime field (when :math:`n = 1`)
* :math:`\mathbb{F}_{p^n}` denotes an extension field (when :math:`n >
  1`)

**Scalar Sequence**:

* :math:`s_0, s_1, s_2, \ldots \in \mathbb{F}_q` denotes the infinite
  sequence generated by the LFSR

**State Vectors**:

* :math:`S_i = (s_i, s_{i+1}, \ldots, s_{i+d-1}) \in \mathbb{F}_q^d`
  denotes the state vector at time :math:`i`
* The state vector is a window of :math:`d` consecutive elements from
  the scalar sequence
* The relationship is: :math:`S_i[j] = s_{i+j}` for :math:`j = 0,
  \ldots, d-1`

**LFSR Parameters**:

* :math:`d` denotes the degree (length) of the LFSR
* :math:`c_0, c_1, \ldots, c_{d-1} \in \mathbb{F}_q` denote the
  feedback coefficients
* The coefficient vector is :math:`[c_0, c_1, \ldots, c_{d-1}]`

**Matrices**:

* :math:`C` denotes the state update matrix (companion matrix)
* :math:`I` denotes the identity matrix
* Matrix multiplication uses row vectors: :math:`S_{i+1} = S_i \cdot
  C`

**Polynomials**:

* :math:`P(t)` denotes the characteristic polynomial of the state
  update matrix
* :math:`f(x)` or :math:`f(t)` denotes general polynomials
  (context-dependent)

**Other Symbols**:

* :math:`\lambda` denotes the period of a sequence
* :math:`\mu` denotes the distance to cycle entry (in cycle detection)
* :math:`L(s)` denotes the linear complexity of sequence :math:`s`

Introduction to Linear Feedback Shift Registers
------------------------------------------------

A **Linear Feedback Shift Register (LFSR)** is a shift register whose
input is a linear function of its previous state. LFSRs are
fundamental building blocks in:

* **Cryptography**: Stream ciphers, key generation, pseudorandom
  number generation
* **Error Detection and Correction**: Cyclic redundancy checks (CRC),
  error-correcting codes
* **Signal Processing**: Scrambling, synchronization
* **Random Number Generation**: Pseudorandom sequences with known
  statistical properties

Mathematical Definition
~~~~~~~~~~~~~~~~~~~~~~~

An LFSR of degree :math:`d` over a finite field :math:`\mathbb{F}_q`
(where :math:`q = p^n` for prime :math:`p` and positive integer
:math:`n`) is defined by:

1. **Scalar sequence**: :math:`s_0, s_1, s_2, \ldots \in \mathbb{F}_q`
   (the infinite sequence generated by the LFSR)

2. **State vector**: :math:`S_i = (s_i, s_{i+1}, \ldots, s_{i+d-1})
   \in \mathbb{F}_q^d` (a window of :math:`d` consecutive elements
   from the scalar sequence)

3. **Feedback coefficients**: :math:`c_0, c_1, \ldots, c_{d-1} \in
   \mathbb{F}_q`

4. **Recurrence relation**: For :math:`i \geq 0`:

   .. math::

      s_{i+d} = c_0 s_i + c_1 s_{i+1} + \cdots + c_{d-1} s_{i+d-1} =
      \sum_{j=0}^{d-1} c_j s_{i+j}

The next state vector is computed by shifting all elements left and
computing the new rightmost element using the linear feedback
function:

.. math::

   S_{i+1} = (s_{i+1}, s_{i+2}, \ldots, s_{i+d-1}, s_{i+d})

Finite Fields (Galois Fields)
------------------------------

LFSRs operate over finite fields, also known as Galois fields, denoted
:math:`\mathbb{F}_q` (where :math:`q = p^n` for prime :math:`p` and
positive integer :math:`n`).

Prime Fields :math:`\mathbb{F}_p`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For prime :math:`p`, the field :math:`\mathbb{F}_p` consists of the
integers :math:`\{0, 1, 2, \ldots, p-1\}` with addition and
multiplication modulo :math:`p`.

.. prf:example:: Prime Field F_2
   :label: ex-prime-field-f2

   :math:`\mathbb{F}_2 = \{0, 1\}` with operations:

   .. math::

      \begin{aligned}
      0 + 0 &= 0, \quad 0 + 1 = 1, \quad 1 + 0 = 1, \quad 1 + 1 = 0 \\
      0 \cdot 0 &= 0, \quad 0 \cdot 1 = 0, \quad 1 \cdot 0 = 0, \quad 1 \cdot 1 = 1
      \end{aligned}

Extension Fields :math:`\mathbb{F}_{p^n}`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For :math:`n > 1`, :math:`\mathbb{F}_{p^n}` is an extension field of
:math:`\mathbb{F}_p` with :math:`p^n` elements. It can be constructed
as:

.. math::

   \mathbb{F}_{p^n} \cong \mathbb{F}_p[x] / \langle f(x) \rangle

where :math:`f(x)` is an irreducible polynomial of degree :math:`n`
over :math:`\mathbb{F}_p`.

.. prf:example:: Extension Field F_4
   :label: ex-extension-field-f4

   :math:`\mathbb{F}_4 = \mathbb{F}_2[x] / \langle x^2 + x +
   1 \rangle` has elements :math:`\{ 0, 1, \alpha, \alpha + 1 \}` where
   :math:`\alpha^2 + \alpha + 1 = 0`.

Field Properties
~~~~~~~~~~~~~~~~

* **Additive group**: :math:`(\mathbb{F}_q, +)` is an abelian group
* **Multiplicative group**: :math:`(\mathbb{F}_q^*, \cdot)` is a
  cyclic group of order :math:`q-1`
* **Primitive element**: There exists :math:`\alpha \in
  \mathbb{F}_q^*` such that :math:`\mathbb{F}_q^* = \{\alpha^0,
  \alpha^1, \ldots, \alpha^{q-2}\}`

Abstract Algebra Foundations
-----------------------------

To understand the deep connections between polynomials, matrices, and
scalars in LFSR analysis, we need fundamental concepts from abstract
algebra. This section provides formal definitions of rings, homomorphisms,
quotient rings, and related structures.

Rings
~~~~~

.. prf:definition:: Ring
   :label: def-ring

   A **ring** :math:`(R, +, \cdot)` is a set :math:`R` equipped with two
   binary operations, addition :math:`+` and multiplication :math:`\cdot`,
   such that:

   1. :math:`(R, +)` is an abelian group (additive group)
   2. Multiplication is associative: :math:`(a \cdot b) \cdot c = a \cdot (b \cdot c)`
   3. Multiplication distributes over addition:
      :math:`a \cdot (b + c) = a \cdot b + a \cdot c` and
      :math:`(a + b) \cdot c = a \cdot c + b \cdot c`

   If there exists an element :math:`1 \in R` such that :math:`1 \cdot a = a \cdot 1 = a`
   for all :math:`a \in R`, then :math:`R` is called a **ring with unity**
   (or **unital ring**).

**Examples**:

- The set of integers :math:`\mathbb{Z}` with usual addition and multiplication
- The polynomial ring :math:`\mathbb{F}[t]` over a field :math:`\mathbb{F}`
- The matrix ring :math:`M_d(\mathbb{F})` of :math:`d \times d` matrices over :math:`\mathbb{F}`
- Any field :math:`\mathbb{F}` (fields are special types of rings)

Ring Homomorphisms
~~~~~~~~~~~~~~~~~~

.. prf:definition:: Ring Homomorphism
   :label: def-ring-homomorphism

   Let :math:`R` and :math:`S` be rings. A function :math:`\phi: R \rightarrow S`
   is called a **ring homomorphism** if it preserves the ring operations:

   1. :math:`\phi(a + b) = \phi(a) + \phi(b)` for all :math:`a, b \in R` (additivity)
   2. :math:`\phi(a \cdot b) = \phi(a) \cdot \phi(b)` for all :math:`a, b \in R` (multiplicativity)
   3. If :math:`R` and :math:`S` are rings with unity, then :math:`\phi(1_R) = 1_S`

   The **kernel** of :math:`\phi` is the set:

   .. math::

      \ker(\phi) = \{a \in R : \phi(a) = 0_S\}

   where :math:`0_S` is the additive identity in :math:`S`.

**Key Properties**:

- The kernel :math:`\ker(\phi)` is an ideal of :math:`R`
- If :math:`\phi` is injective (one-to-one), then :math:`\ker(\phi) = \{0\}`
- If :math:`\phi` is surjective (onto), then :math:`\phi` is called an **epimorphism**
- If :math:`\phi` is bijective (one-to-one and onto), then :math:`\phi` is called an **isomorphism**

Ideals and Quotient Rings
~~~~~~~~~~~~~~~~~~~~~~~~~

.. prf:definition:: Ideal
   :label: def-ideal

   Let :math:`R` be a ring. A subset :math:`I \subseteq R` is called an
   **ideal** if:

   1. :math:`I` is a subgroup of :math:`(R, +)` (the additive group)
   2. For all :math:`r \in R` and :math:`i \in I`, we have :math:`r \cdot i \in I`
      and :math:`i \cdot r \in I` (absorption property)

   If :math:`I = \langle a \rangle = \{r \cdot a : r \in R\}` for some
   :math:`a \in R`, then :math:`I` is called a **principal ideal**
   generated by :math:`a`.

.. prf:definition:: Quotient Ring
   :label: def-quotient-ring

   Let :math:`R` be a ring and :math:`I` an ideal of :math:`R`. The
   **quotient ring** :math:`R/I` is the set of equivalence classes
   :math:`[r] = \{r + i : i \in I\}` for :math:`r \in R`, with operations:

   - :math:`[a] + [b] = [a + b]`
   - :math:`[a] \cdot [b] = [a \cdot b]`

   The elements :math:`[a]` and :math:`[b]` are considered equal (written
   :math:`a \equiv b \pmod{I}`) if :math:`a - b \in I`.

**Example**: For the polynomial ring :math:`\mathbb{F}[t]` and an ideal
:math:`I = \langle P(t) \rangle` generated by a polynomial :math:`P(t)`,
the quotient ring :math:`\mathbb{F}[t] / \langle P(t) \rangle` consists
of all polynomials modulo :math:`P(t)`. Two polynomials are equivalent if
their difference is divisible by :math:`P(t)`.

Isomorphisms
~~~~~~~~~~~~

.. prf:definition:: Ring Isomorphism
   :label: def-ring-isomorphism

   A **ring isomorphism** is a bijective ring homomorphism. If
   :math:`\phi: R \rightarrow S` is an isomorphism, we say that :math:`R`
   and :math:`S` are **isomorphic**, denoted :math:`R \cong S`.

   Isomorphic rings have identical algebraic structure—they differ only in
   the names of their elements. Any algebraic property true in one ring
   automatically holds in an isomorphic ring.

**First Isomorphism Theorem for Rings**:

If :math:`\phi: R \rightarrow S` is a ring homomorphism, then:

.. math::

   R / \ker(\phi) \cong \text{im}(\phi)

where :math:`\text{im}(\phi) = \{\phi(r) : r \in R\}` is the image of
:math:`\phi`.

This theorem is fundamental: it states that the quotient ring by the
kernel is isomorphic to the image of the homomorphism.

Evaluation Homomorphism
~~~~~~~~~~~~~~~~~~~~~~~

.. prf:definition:: Evaluation Homomorphism
   :label: def-evaluation-homomorphism

   Let :math:`\mathbb{F}[t]` be the polynomial ring over a field
   :math:`\mathbb{F}`, and let :math:`A` be an algebra over
   :math:`\mathbb{F}` (e.g., the matrix ring :math:`M_d(\mathbb{F})`).
   For a fixed element :math:`a \in A`, the **evaluation homomorphism**
   :math:`\phi_a: \mathbb{F}[t] \rightarrow A` is defined by:

   .. math::

      \phi_a(f(t)) = f(a)

   where :math:`f(a)` means substituting :math:`a` for the variable
   :math:`t` in the polynomial :math:`f(t)`.

**Properties**:

- :math:`\phi_a` is indeed a ring homomorphism
- The kernel :math:`\ker(\phi_a)` consists of all polynomials :math:`f(t)`
  such that :math:`f(a) = 0`
- If :math:`P(t)` is the minimal polynomial of :math:`a`, then
  :math:`\ker(\phi_a) = \langle P(t) \rangle`
- By the First Isomorphism Theorem:
  :math:`\mathbb{F}[t] / \ker(\phi_a) \cong \text{im}(\phi_a)`

**Application to LFSR**: For a companion matrix :math:`C`, the evaluation
homomorphism :math:`\phi_C: \mathbb{F}[t] \rightarrow M_d(\mathbb{F})`
maps polynomials to matrices. The Cayley-Hamilton theorem tells us that
the characteristic polynomial :math:`P(t)` lies in :math:`\ker(\phi_C)`,
establishing the connection between polynomial and matrix rings.

State Update Matrix and Companion Matrix
----------------------------------------

Matrix Representation
~~~~~~~~~~~~~~~~~~~~~~

The state transition of an LFSR can be represented as matrix
multiplication. Given state vector :math:`S_i = (s_i, s_{i+1}, \ldots,
s_{i+d-1})`, the next state is:

.. math::

   S_{i+1} = S_i \cdot C

where :math:`C` is the **state update matrix** (also called the
**companion matrix**), and :math:`S_i` is treated as a row vector.

Companion Matrix Construction
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For LFSR with coefficients :math:`c_0, c_1, \ldots, c_{d-1}`, the
companion matrix :math:`C` is a :math:`d \times d` matrix over
:math:`\mathbb{F}_q` with the following structure:

.. math::

   C = \begin{pmatrix}
   0 & 0 & 0 & \cdots & c_0 \\
   1 & 0 & 0 & \cdots & c_1 \\
   0 & 1 & 0 & \cdots & c_2 \\
   \vdots & \vdots & \ddots & \ddots & \vdots \\
   0 & 0 & \cdots & 1 & c_{d-1}
   \end{pmatrix}

**Structure**:

* **Subdiagonal**: For :math:`i = 1, 2, \ldots, d-1`, the entry at
  position :math:`(i, i-1)` is 1
* **Last column** (column :math:`d-1`): Contains the LFSR feedback coefficients
  :math:`c_0, c_1, \ldots, c_{d-1}` at positions :math:`(i, d-1)` for
  :math:`i = 0, \ldots, d-1`
* **All other entries**: 0

**Critical Note**: Coefficients are stored in the **last column**
(column :math:`d-1`), not the last row. This is essential for parallel
processing where coefficients must be extracted for matrix
reconstruction in worker processes using :math:`c_i = C[i, d-1]` for
:math:`i = 0, \ldots, d-1`.

**Proof of Correctness**:

Let :math:`S_i = (s_i, s_{i+1}, \ldots, s_{i+d-1})`. Then:

.. math::

   S_{i+1} = S_i \cdot C = (s_i, s_{i+1}, \ldots, s_{i+d-1}) \cdot C

Computing the product (row vector times matrix):

.. math::

   \begin{aligned}
   (S_i \cdot C)_0 &= s_i \cdot 0 + s_{i+1} \cdot 1 + s_{i+2} \cdot 0 + \cdots + s_{i+d-1} \cdot 0 = s_{i+1} \\
   (S_i \cdot C)_1 &= s_i \cdot 0 + s_{i+1} \cdot 0 + s_{i+2} \cdot 1 + \cdots + s_{i+d-1} \cdot 0 = s_{i+2} \\
   &\vdots \\
   (S_i \cdot C)_{d-2} &= s_{i+d-1} \\
   (S_i \cdot C)_{d-1} &= s_i \cdot c_0 + s_{i+1} \cdot c_1 + \cdots + s_{i+d-1} \cdot c_{d-1} = \sum_{j=0}^{d-1} c_j s_{i+j}
   \end{aligned}

By the recurrence relation:

.. math::

   s_{i+d} = \sum_{j=0}^{d-1} c_j s_{i+j}

Therefore:

.. math::

   S_{i+1} = (s_{i+1}, s_{i+2}, \ldots, s_{i+d-1}, s_{i+d})

The matrix :math:`C` correctly implements the shift operation (moving
elements left) and computes the new rightmost element using the linear
feedback function.

.. prf:example:: Companion Matrix Construction
   :label: ex-companion-matrix

   For LFSR with coefficients :math:`[1, 1, 0, 0]` over
   :math:`\mathbb{F}_2` (this gives the primitive polynomial :math:`t^4 + t + 1`):

   .. math::

      C = \begin{pmatrix}
      0 & 0 & 0 & 1 \\
      1 & 0 & 0 & 1 \\
      0 & 1 & 0 & 0 \\
      0 & 0 & 1 & 0
      \end{pmatrix}

   Given state :math:`S_0 = (s_0, s_1, s_2, s_3) = (1, 0, 0, 0)`:

   .. math::

      S_1 = S_0 \cdot C = (1, 0, 0, 0) \cdot C = (0, 0, 0, 1)

   where the last element is computed as :math:`s_4 = 1 \cdot 1 + 0 \cdot
   1 + 0 \cdot 0 + 0 \cdot 0 = 1`.

Characteristic Polynomial
-------------------------

Definition
~~~~~~~~~~

The **characteristic polynomial** of the state update matrix :math:`C`
is:

.. math::

   P(t) = \det(tI - C)

where :math:`I` is the :math:`d \times d` identity matrix.

Computation
~~~~~~~~~~~

For the companion matrix :math:`C`, the characteristic polynomial is:

.. math::

   P(t) = t^d - c_{d-1} t^{d-1} - c_{d-2} t^{d-2} - \cdots - c_1 t - c_0

**Note**: In fields where :math:`-1 \neq 1` (i.e., :math:`q > 2`), the
formula uses subtraction. In :math:`\mathbb{F}_2`, since :math:`-1 =
1`, the formula becomes:

.. math::

   P(t) = t^d + c_{d-1} t^{d-1} + c_{d-2} t^{d-2} + \cdots + c_1 t + c_0

.. prf:proof::

   Let :math:`C` be the companion matrix as defined above. Then:

   .. math::

      tI - C = \begin{pmatrix}
      t & 0 & 0 & \cdots & -c_0 \\
      -1 & t & 0 & \cdots & -c_1 \\
      0 & -1 & t & \cdots & -c_2 \\
      \vdots & \vdots & \ddots & \ddots & \vdots \\
      0 & 0 & \cdots & -1 & t - c_{d-1}
      \end{pmatrix}

   We compute :math:`\det(tI - C)` by expanding along the first column.
   The first column has entries :math:`t` at position :math:`(0,0)`,
   :math:`-1` at position :math:`(1,0)`, and zeros elsewhere. By the
   cofactor expansion formula:

   .. math::

      \det(tI - C) = t \cdot \det(M_0) - (-1) \cdot \det(M_1)

   where :math:`M_0` is the :math:`(d-1) \times (d-1)` submatrix obtained
   by deleting the first row and column, and :math:`M_1` is the
   :math:`(d-1) \times (d-1)` submatrix obtained by deleting the second
   row and first column.

   The submatrix :math:`M_0` has the same structure as :math:`tI - C` but
   of dimension :math:`d-1`. By induction, we obtain:

   .. math::

      \det(M_0) = t^{d-1} - c_{d-1} t^{d-2} - c_{d-2} t^{d-3} - \cdots - c_2 t - c_1

   The submatrix :math:`M_1` is lower triangular with :math:`-1` on the
   diagonal, hence :math:`\det(M_1) = (-1)^{d-1}`.

   Substituting into the expansion:

   .. math::

      \begin{aligned}
      \det(tI - C) &= t \cdot (t^{d-1} - c_{d-1} t^{d-2} - \cdots - c_2 t - c_1) - (-1) \cdot (-1)^{d-1} \\
                   &= t^d - c_{d-1} t^{d-1} - c_{d-2} t^{d-2} - \cdots - c_1 t - c_0
      \end{aligned}

   This establishes the desired result.

.. prf:example:: Characteristic Polynomial Computation
   :label: ex-char-poly-computation

   For :math:`C` with coefficients :math:`[1, 1, 0, 0]` over
   :math:`\mathbb{F}_2`:

   From the companion matrix:

   .. math::

      C = \begin{pmatrix}
      0 & 0 & 0 & 1 \\
      1 & 0 & 0 & 1 \\
      0 & 1 & 0 & 0 \\
      0 & 0 & 1 & 0
      \end{pmatrix}

   We compute:

   .. math::

      tI - C = \begin{pmatrix}
      t & 0 & 0 & -1 \\
      -1 & t & 0 & -1 \\
      0 & -1 & t & 0 \\
      0 & 0 & -1 & t
      \end{pmatrix}

   Using the general formula :math:`P(t) = t^d - c_{d-1} t^{d-1} -
   \cdots - c_0`:

   .. math::

      P(t) = t^4 - c_3 t^3 - c_2 t^2 - c_1 t - c_0 = t^4 - 0 \cdot t^3 -
      0 \cdot t^2 - 1 \cdot t - 1

   Since we are in :math:`\mathbb{F}_2` where :math:`-1 = 1`, this
   simplifies to:

   .. math::

      P(t) = t^4 + t + 1

   This polynomial is **primitive** and has order 15, giving maximum
   period.

Cayley-Hamilton Theorem
~~~~~~~~~~~~~~~~~~~~~~~

The characteristic polynomial :math:`P(t) = \det(tI - C)` encodes
fundamental information about the matrix :math:`C`. Remarkably, this
polynomial relationship extends beyond the scalar variable :math:`t` to
the matrix :math:`C` itself. The Cayley-Hamilton theorem establishes
this profound connection, revealing that matrices satisfy their own
characteristic equations.

**Theoretical Motivation: A Ring-Theoretic Perspective**

The Cayley-Hamilton theorem reveals a profound unification through the
lens of abstract algebra. Consider three mathematical structures:

1. **The polynomial ring** :math:`\mathbb{F}[t]` of polynomials in the
   indeterminate :math:`t` over the field :math:`\mathbb{F}`

2. **The matrix ring** :math:`M_d(\mathbb{F})` of :math:`d \times d`
   matrices over :math:`\mathbb{F}`

3. **The field** :math:`\mathbb{F}` itself (viewed as scalars)

The power of abstract algebra lies in recognizing that these seemingly
different objects can be treated uniformly within the framework of
**rings**. The Cayley-Hamilton theorem demonstrates that polynomials,
scalars, and matrices are not fundamentally different—they are different
representations of the same abstract algebraic structure.

**The Evaluation Homomorphism**

The key insight is the **evaluation map** :math:`\phi: \mathbb{F}[t]
\rightarrow M_d(\mathbb{F})` defined by :math:`\phi(f(t)) = f(C)`,
where :math:`f(C)` means substituting the matrix :math:`C` for the
variable :math:`t`. This map is a **ring homomorphism**, meaning it
preserves the ring operations:

- :math:`\phi(f + g) = \phi(f) + \phi(g)` (additivity)
- :math:`\phi(f \cdot g) = \phi(f) \cdot \phi(g)` (multiplicativity)
- :math:`\phi(1) = I` (preserves identity)

The characteristic polynomial :math:`P(t)` captures the eigenvalues of
:math:`C` as its roots in :math:`\mathbb{F}`. The Cayley-Hamilton
theorem asserts that this polynomial relationship extends to the matrix
ring: :math:`P(C) = 0` in :math:`M_d(\mathbb{F})`. This reveals that
:math:`P(t)` lies in the **kernel** of the evaluation homomorphism,
establishing a fundamental connection between the polynomial ring and
the matrix ring.

**Intuition**

Consider evaluating a polynomial :math:`f(t)` at a matrix :math:`C` by
substituting :math:`t = C`. The Cayley-Hamilton theorem states that
when we evaluate the characteristic polynomial :math:`P(t)` at
:math:`C`, we obtain the zero matrix. This means :math:`C` is a "root"
of its characteristic polynomial in the matrix sense, analogous to how
eigenvalues are roots in the scalar sense.

.. prf:theorem:: Cayley-Hamilton Theorem
   :label: theorem-cayley-hamilton

   Let :math:`C` be a :math:`d \times d` matrix over a field
   :math:`\mathbb{F}`, and let :math:`P(t) = \det(tI - C) = t^d - c_{d-1}
   t^{d-1} - \cdots - c_1 t - c_0` be its characteristic polynomial.
   Then :math:`C` satisfies its own characteristic equation:

   .. math::

      P(C) = C^d - c_{d-1} C^{d-1} - \cdots - c_1 C - c_0 I = 0

   where :math:`0` denotes the :math:`d \times d` zero matrix.

.. prf:proof::

   The proof relies on the adjugate matrix. Let :math:`\text{adj}(tI - C)`
   denote the adjugate of :math:`tI - C`. By the fundamental identity:

   .. math::

      (tI - C) \cdot \text{adj}(tI - C) = \det(tI - C) \cdot I = P(t) \cdot I

   The adjugate :math:`\text{adj}(tI - C)` is a matrix whose entries are
   polynomials in :math:`t` of degree at most :math:`d-1`. Therefore, we
   can write:

   .. math::

      \text{adj}(tI - C) = B_0 + B_1 t + B_2 t^2 + \cdots + B_{d-1} t^{d-1}

   for some :math:`d \times d` matrices :math:`B_0, B_1, \ldots, B_{d-1}`
   over :math:`\mathbb{F}`.

   Substituting into the identity and comparing coefficients of powers of
   :math:`t`, we obtain:

   .. math::

      P(t) I = (tI - C)(B_0 + B_1 t + \cdots + B_{d-1} t^{d-1})

   Expanding and equating coefficients yields a system of matrix equations.
   When we substitute :math:`t = C` into this polynomial identity, the
   left-hand side becomes :math:`P(C)`, while the right-hand side
   contains the factor :math:`(CI - C) = 0`, forcing :math:`P(C) = 0`.

   This establishes the theorem.

**Consequences and Applications**

The Cayley-Hamilton theorem has profound implications for matrix
analysis:

1. **Matrix Powers**: Any power :math:`C^k` with :math:`k \geq d` can be
   expressed as a linear combination of :math:`I, C, C^2, \ldots,
   C^{d-1}`. This follows by repeatedly applying the identity
   :math:`C^d = c_{d-1} C^{d-1} + \cdots + c_1 C + c_0 I`.

2. **Matrix Functions**: For any polynomial :math:`f(t)`, the evaluation
   :math:`f(C)` can be computed using polynomial division: write
   :math:`f(t) = q(t) P(t) + r(t)` where :math:`\deg(r) < d`, then
   :math:`f(C) = r(C)`.

3. **Minimal Polynomial**: The characteristic polynomial provides an
   upper bound on the degree of the minimal polynomial, which divides
   :math:`P(t)`.

4. **LFSR Analysis**: For the companion matrix :math:`C`, the theorem
   establishes that the state space has dimension at most :math:`d` in
   the sense that all matrix powers lie in the span of
   :math:`\{I, C, \ldots, C^{d-1}\}`. This is fundamental for computing
   the order of :math:`C` and understanding sequence periodicity.

5. **Computational Efficiency**: Instead of computing :math:`C^n` directly
   (which requires :math:`O(d^3 \log n)` operations), we can use the
   recurrence relation to compute it in :math:`O(d^2 \log n)` operations
   by expressing :math:`C^n` as a polynomial in :math:`C` of degree
   less than :math:`d`.

**Unification Through Abstract Algebra**

The ring-theoretic perspective reveals why the Cayley-Hamilton theorem
is so powerful: it demonstrates that **polynomials, scalars, and
matrices are different representations of the same abstract structure**.

Consider the following unified treatment:

- **Scalar evaluation**: For an eigenvalue :math:`\lambda \in \mathbb{F}`,
  we have :math:`P(\lambda) = 0` (by definition of eigenvalue)

- **Polynomial evaluation**: The polynomial :math:`P(t) \in \mathbb{F}[t]`
  encodes the algebraic relationship

- **Matrix evaluation**: The Cayley-Hamilton theorem states
  :math:`P(C) = 0` in :math:`M_d(\mathbb{F})`

All three statements express the same fundamental relationship, but in
different rings. The evaluation homomorphism :math:`\phi: \mathbb{F}[t]
\rightarrow M_d(\mathbb{F})` provides the bridge, showing that what is
true for scalars (eigenvalues) extends naturally to matrices through
the ring structure.

**The Power of Representation Independence**

This abstraction is the essence of linear algebra's power: we can work
with polynomials, compute with scalars, and reason about matrices, all
while operating within the same algebraic framework. The Cayley-Hamilton
theorem is not merely a computational tool—it is a manifestation of
the deep structural unity that abstract algebra reveals.

The fact that the same polynomial :math:`P(t)` can be evaluated at
scalars, treated as an abstract algebraic object, or evaluated at
matrices, demonstrates the **representation independence** that makes
abstract algebra so powerful. We are not working with three different
things—we are working with one thing (the polynomial) in three
different contexts (scalar field, polynomial ring, matrix ring).

**Connection to LFSR Periodicity**

For LFSR analysis, this ring-theoretic perspective provides the
theoretical foundation for understanding why sequence periods are bounded
and how they relate to the characteristic polynomial. The recurrence
relation :math:`C^d = c_{d-1} C^{d-1} + \cdots + c_1 C + c_0 I` directly
corresponds to the LFSR recurrence relation, establishing the deep
connection between the algebraic structure of the polynomial ring and
the dynamical behavior of the state sequence in the matrix ring.

The fact that :math:`P(C) = 0` means that in the quotient ring
:math:`M_d(\mathbb{F}) / \langle P(C) \rangle`, all powers of :math:`C`
reduce to linear combinations of :math:`I, C, \ldots, C^{d-1}`, which
directly explains the periodicity and boundedness of LFSR sequences.

Polynomial Order
----------------

Definition
~~~~~~~~~~

The **order** of a polynomial :math:`P(t)` over :math:`\mathbb{F}_q`
is the smallest positive integer :math:`n` such that:

.. math::

   t^n \equiv 1 \pmod{P(t)}

If no such :math:`n` exists (within the search space), the order is
infinite.

Connection to Matrix Order
~~~~~~~~~~~~~~~~~~~~~~~~~~

The relationship between polynomial order and matrix order exemplifies
the power of abstract algebra in unifying different mathematical
structures. Through the evaluation homomorphism :math:`\phi: \mathbb{F}[t]
\rightarrow M_d(\mathbb{F})`, we can translate questions about
polynomials into questions about matrices, and vice versa.

**Theoretical Framework**

Recall that the evaluation homomorphism :math:`\phi(f(t)) = f(C)` maps
polynomials to matrices. The Cayley-Hamilton theorem tells us that
:math:`P(t)` lies in the kernel of this homomorphism, i.e.,
:math:`\phi(P(t)) = P(C) = 0`. This means that in the quotient ring
:math:`\mathbb{F}[t] / \langle P(t) \rangle`, the polynomial :math:`t`
corresponds to the matrix :math:`C` under the induced isomorphism.

The order of :math:`P(t)` is the smallest :math:`n` such that
:math:`t^n \equiv 1 \pmod{P(t)}` in the quotient ring
:math:`\mathbb{F}[t] / \langle P(t) \rangle`. The order of :math:`C`
is the smallest :math:`n` such that :math:`C^n = I` in the matrix ring
:math:`M_d(\mathbb{F})`. The following theorem establishes their
equivalence.

.. prf:theorem:: Order Equivalence
   :label: theorem-order-equivalence

   Let :math:`C` be a companion matrix with characteristic polynomial
   :math:`P(t) = \det(tI - C)`. Then the order of :math:`P(t)` in the
   quotient ring :math:`\mathbb{F}[t] / \langle P(t) \rangle` equals the
   order of :math:`C` in the multiplicative semigroup of
   :math:`M_d(\mathbb{F})`.

.. prf:proof::

   Let :math:`n` be the order of :math:`C`, so :math:`C^n = I`. Consider
   the evaluation homomorphism :math:`\phi: \mathbb{F}[t] \rightarrow
   M_d(\mathbb{F})` defined by :math:`\phi(f(t)) = f(C)`.

   Since :math:`\phi(t^n) = C^n = I` and :math:`\phi(1) = I`, we have
   :math:`\phi(t^n - 1) = 0`. By the Cayley-Hamilton theorem,
   :math:`\phi(P(t)) = P(C) = 0`, so :math:`P(t)` lies in the kernel of
   :math:`\phi`.

   The fact that :math:`\phi(t^n - 1) = 0` means that :math:`t^n - 1` is
   also in the kernel. Since the kernel is an ideal containing
   :math:`P(t)`, and :math:`t^n - 1` evaluates to zero, we must have
   :math:`P(t) \mid (t^n - 1)` in :math:`\mathbb{F}[t]`, or equivalently,
   :math:`t^n \equiv 1 \pmod{P(t)}`.

   Since :math:`n` is the smallest positive integer with :math:`C^n = I`,
   and :math:`C^k \neq I` for any :math:`k < n`, we have
   :math:`\phi(t^k - 1) = C^k - I \neq 0` for :math:`k < n`. This implies
   :math:`t^k \not\equiv 1 \pmod{P(t)}` for :math:`k < n`. Therefore,
   :math:`n` is the smallest positive integer with :math:`t^n \equiv 1
   \pmod{P(t)}`, establishing that the order of :math:`P(t)` is :math:`n`.

   Conversely, suppose :math:`n` is the order of :math:`P(t)`, so
   :math:`t^n \equiv 1 \pmod{P(t)}`. Then there exists a polynomial
   :math:`q(t)` such that :math:`t^n - 1 = q(t) P(t)`. Applying the
   evaluation homomorphism:

   .. math::

      \phi(t^n - 1) = \phi(q(t)) \cdot \phi(P(t)) = \phi(q(t)) \cdot 0 = 0

   Therefore, :math:`C^n - I = 0`, so :math:`C^n = I`. If there existed
   :math:`k < n` with :math:`C^k = I`, then by the same argument we would
   have :math:`t^k \equiv 1 \pmod{P(t)}`, contradicting the minimality
   of :math:`n`. Hence, :math:`n` is the order of :math:`C`.

   This establishes the equivalence.

**Algebraic Interpretation**

This theorem reveals a deep structural connection: the multiplicative
order of :math:`t` in the quotient ring :math:`\mathbb{F}[t] / \langle
P(t) \rangle` (which is a field when :math:`P(t)` is irreducible)
corresponds exactly to the multiplicative order of :math:`C` in the
matrix ring. This is a manifestation of the fact that the evaluation
homomorphism induces an isomorphism between the quotient ring and the
subalgebra generated by :math:`C`.

**Computational Implications**

This equivalence provides powerful computational tools:

- We can compute matrix order by working in the polynomial ring, where
  we can use efficient polynomial arithmetic and modular reduction

- Conversely, we can verify polynomial order properties by computing
  matrix powers, which may be more efficient in certain contexts

- The connection enables us to leverage results from finite field theory
  (where polynomial orders are well-studied) to understand matrix
  behavior

.. prf:example:: Polynomial Order Computation
   :label: ex-poly-order

   For the primitive polynomial :math:`P(t) = t^4 + t + 1` over
   :math:`\mathbb{F}_2`:

   We check :math:`t^n \bmod P(t)` for increasing :math:`n`:

   * :math:`t^1 = t \not\equiv 1`
   * :math:`t^2 = t^2 \not\equiv 1`
   * :math:`t^3 = t^3 \not\equiv 1`
   * :math:`t^4 = t + 1 \not\equiv 1` (since :math:`t^4 = P(t) - (t + 1)
     = (t^4 + t + 1) - (t + 1)`)
   * :math:`t^5 = t \cdot t^4 = t(t + 1) = t^2 + t \not\equiv 1`
   * ... (continuing) ...
   * :math:`t^{15} \equiv 1 \pmod{P(t)}`

   So the order is 15, which equals :math:`2^4 - 1 = 15`, confirming that
   :math:`P(t)` is primitive.

Period and Sequence Analysis
-----------------------------

Matrix Order
~~~~~~~~~~~~

The **order** (or **period**) of matrix :math:`C` is the smallest
positive integer :math:`n` such that:

.. math::

   C^n = I

This represents the maximum period before any state sequence repeats.

**Properties**:

1. **Upper Bound**: The order :math:`n \leq q^d - 1` (by the
   pigeonhole principle)
2. **Divisibility**: The order divides :math:`q^d - 1` (by group
   theory)
3. **Maximal Period**: If :math:`P(t)` is primitive, then :math:`n =
   q^d - 1` (maximum possible)

State Sequence Periods
~~~~~~~~~~~~~~~~~~~~~~

For a given initial state :math:`S_0`, the sequence :math:`S_0, S_1,
S_2, \ldots` is periodic. The **period** of this sequence is the
smallest :math:`k` such that :math:`S_k = S_0`.

.. prf:theorem:: Period Divisibility
   :label: theorem-period-divisibility

   The period of a state sequence divides the order of the matrix.

.. prf:proof::

   If :math:`C^n = I`, then for any state :math:`S_0`:

.. math::

   S_n = S_0 \cdot C^n = S_0 \cdot I = S_0

So the sequence repeats after :math:`n` steps. The period :math:`k`
must divide :math:`n` (if :math:`k` didn't divide :math:`n`, we could
find a smaller period, contradicting minimality).

.. prf:example:: Period Divisibility
   :label: ex-period-divisibility

   For the 4-bit LFSR with primitive polynomial
   :math:`P(t) = t^4 + t + 1` over :math:`\mathbb{F}_2`:

   * Matrix order: 15
   * Possible sequence periods: 1, 3, 5, 15 (divisors of 15)
   * State :math:`(0,0,0,0)` has period 1 (all-zero state)
   * All other non-zero states have period 15 (maximum period)

Sequence Mapping Algorithm
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The algorithm for finding all sequences:

1. **Initialize**: Start with all :math:`q^d` possible state vectors
2. **Traverse**: For each unvisited state :math:`S_0`:
   
   a. Find the cycle period using cycle detection (see below)
   b. Generate the full sequence :math:`S_0, S_1, S_2, \ldots` using
      :math:`S_{i+1} = S_i \cdot C`
   c. Track visited states to avoid reprocessing
   d. Record period :math:`k` and sequence
3. **Categorize**: Group states by their sequence cycles

**Complexity**: :math:`O(q^d)` time and space (visiting each state
 once).

Cycle Detection Algorithms
~~~~~~~~~~~~~~~~~~~~~~~~~~

Finding the period of a state sequence is a fundamental operation. Two
main approaches are used:

**Naive Enumeration Method**:
The straightforward approach enumerates all states until the cycle is
detected:

.. math::

   \begin{aligned}
   S_0, S_1 = S_0 \cdot C, S_2 = S_1 \cdot C, \ldots, S_k = S_{k-1} \cdot C
   \end{aligned}

Continue until :math:`S_k = S_0`. The period is :math:`k`.

**Complexity**: 
* Time: :math:`O(\lambda)` where :math:`\lambda` is the period
* Space: :math:`O(\lambda)` to store all states in the cycle

**Floyd's Cycle Detection Algorithm (Tortoise and Hare)**:
An algorithm that can find the period using only :math:`O(1)` extra
space for the period-finding phase.
Note: Our implementation still uses :math:`O(\lambda)` space because
we need to store the full sequence for output.

**Algorithm Description**:

1. **Phase 1 - Find Meeting Point**:
   Start two pointers (tortoise and hare) at the initial state
   :math:`S_0`.
   Move tortoise one step: :math:`T_{i+1} = T_i \cdot C`
   Move hare two steps: :math:`H_{i+1} = (H_i \cdot C) \cdot C`
   Continue until they meet: :math:`T_j = H_j` for some :math:`j`.

2. **Phase 2 - Find Period**:
   Reset tortoise to :math:`S_0`, keep hare at meeting point.
   Move both one step at a time: :math:`T_{i+1} = T_i \cdot C`,
   :math:`H_{i+1} = H_i \cdot C`
   Count steps until they meet again. The number of steps
   :math:`\lambda` is the period.

**Mathematical Proof**:

Let :math:`\mu` be the index where the cycle starts (distance from
:math:`S_0` to cycle entry) and :math:`\lambda` be the period.

**Phase 1**: After :math:`i` iterations:
* Tortoise position: :math:`S_i`
* Hare position: :math:`S_{2i}`

They meet when :math:`S_i = S_{2i}`. Since the sequence is periodic:

* :math:`S_i = S_{\mu + ((i-\mu) \bmod \lambda)}`
* :math:`S_{2i} = S_{\mu + ((2i-\mu) \bmod \lambda)}`

For :math:`S_i = S_{2i}`, we need:

.. math::

   \mu + ((i-\mu) \bmod \lambda) = \mu + ((2i-\mu) \bmod \lambda)

This implies :math:`i \equiv 2i \pmod{\lambda}`, so :math:`i \equiv 0
\pmod{\lambda}`.
The smallest such :math:`i` is a multiple of :math:`\lambda`, and
:math:`i \geq \mu`.

**Phase 2**: After resetting tortoise to :math:`S_0` and moving both one step:

* Tortoise: :math:`S_k` for :math:`k = 0, 1, 2, \ldots`
* Hare: :math:`S_{i+k}` where :math:`i` is the meeting point from Phase 1

They meet when :math:`S_k = S_{i+k}`. Since :math:`i` is a multiple of
:math:`\lambda`, :math:`S_{i+k} = S_k`, so they meet when :math:`k =
\mu` (tortoise enters cycle).
The period :math:`\lambda` is found by counting steps from this
meeting point until the next meeting.

**Theoretical Complexity**:

* Time: :math:`O(\lambda)` - same as enumeration
* Space for period finding: :math:`O(1)` - only stores two state
  pointers

**Practical Implementation**:

* Our period-only implementation achieves true :math:`O(1)` space
  (verified by profiling)
* Full sequence mode stores the sequence, so space is
  :math:`O(\lambda)` (same as enumeration)
* Performance varies significantly by input - enumeration is typically
  faster

**Performance Characteristics** (Period-Only Mode):

* **Operation Count**: Floyd performs approximately **3.83× more
  matrix operations** than enumeration

  * Phase 1: Tortoise moves :math:`\sim \lambda/2` steps, hare moves
    :math:`2 \times \lambda/2 = \lambda` steps
  * Phase 2: Additional :math:`\lambda` steps to find period
  * Total: :math:`\sim 3 \times \lambda/2 + \lambda = 2.5\lambda`
    operations vs :math:`\lambda` for enumeration

* **Time Performance**: Enumeration is typically **3-5× faster** for
  periods < 1000

  * Floyd overhead (Phase 1 + Phase 2) dominates for small-to-medium
    periods
  * Time per operation is similar (~0.022 ms), so speed difference
    comes from operation count

* **Memory**: Both achieve true :math:`O(1)` space in period-only mode
  
  * Floyd: ~1.60 KB (constant, verified across iterations)
  * Enumeration: ~1.44 KB (constant, verified across iterations)
  * Memory usage is independent of period size ✓

**When Floyd is Beneficial**:

* **Very Large Periods** (> 10,000): Overhead might be amortized
  (needs testing)
* **Educational/Verification**: Using different algorithm to verify
  results
* **Memory-Constrained**: If enumeration had memory issues (but it
  doesn't in period-only mode)
* **Parallel Processing**: Floyd's structure might be more
  parallelizable (future work)

**When Enumeration is Better**:

* **Small-to-Medium Periods** (< 1000): Simpler, faster, uses less
  memory
* **Typical LFSR Analysis**: Most practical use cases
* **Default Choice**: Recommended for most scenarios

.. prf:example:: Floyd's Cycle Detection
   :label: ex-floyd-cycle

   For an LFSR with period 15:

   Phase 1: Tortoise and hare start at :math:`S_0`:

   * Step 0: :math:`T = S_0`, :math:`H = S_0`
   * Step 1: :math:`T = S_1`, :math:`H = S_2`
   * Step 2: :math:`T = S_2`, :math:`H = S_4`
   * ...
   * They meet at some point in the cycle

   Phase 2: Reset tortoise, move both one step:

   * Find the period by counting steps until they meet again

**Implementation Note**:

* **Full Sequence Mode**: Enumeration is the default (faster,
  simpler). Both algorithms use :math:`O(\lambda)` space since
  sequences must be stored.
* **Period-Only Mode** (``--period-only``): Enumeration is default,
  Floyd available as option. Both achieve true :math:`O(1)` space, but
  enumeration is typically 3-5× faster.
* Use ``--algorithm`` to select algorithm (floyd, brent, enumeration,
  or auto), or ``scripts/performance_profile.py`` for detailed
  analysis.
* See :ref:`performance-analysis` for comprehensive performance
  discussion.

Brent's Cycle Detection Algorithm
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Brent's cycle detection algorithm is an alternative to Floyd's
algorithm that uses powers of 2 to find cycles. Like Floyd's, it finds
the period :math:`\lambda` in :math:`O(\lambda)` time using only
:math:`O(1)` extra space.

**Algorithm Description**:

1. Initialize tortoise and hare to the starting state
2. Use a "power" variable that doubles (powers of 2: 1, 2, 4, 8, ...)
3. Move the hare forward, incrementing the period counter
4. When the period counter reaches the current power:
   
   * Reset the tortoise to the hare's position
   * Double the power
   * Reset the period counter
5. Continue until tortoise and hare meet (cycle detected)
6. The period counter gives the cycle length

**Key Differences from Floyd's**:

* Uses powers of 2 instead of moving at different speeds
* Can be more efficient in some cases due to fewer state comparisons
* Similar time complexity but different operation pattern

**Performance Characteristics**:

* Similar to Floyd's algorithm in terms of operation count and time
* Both perform ~4× more operations than enumeration
* Both achieve true O(1) space in period-only mode
* Enumeration is typically 3-5× faster for typical periods

**When to Use**:

* Educational/verification purposes (alternative to Floyd's)
* When you want to compare different cycle detection methods
* Similar use cases as Floyd's algorithm

Parallel State Enumeration
---------------------------

For large LFSRs with state spaces containing thousands or millions of
states, sequential processing can be time-consuming. Parallel state
enumeration partitions the state space across multiple CPU cores to
achieve significant speedup on multi-core systems. The implementation
uses fork mode (13-17x faster than spawn) with SageMath isolation to
provide 2-4x speedup for large LFSRs.

**Motivation**:

The sequential algorithm processes states one at a time, which is
efficient for small LFSRs but becomes a bottleneck for large state
spaces. For an LFSR with :math:`q^d` states, sequential processing
requires :math:`O(q^d)` time. On a multi-core system with :math:`n`
cores, we can theoretically achieve up to :math:`n`-fold speedup by
processing states in parallel.

**Architecture**:

The parallel implementation provides two modes:

**Static Partitioning (Fixed Work Distribution)**:

1. **State Space Partitioning**: The entire state space is divided into
   :math:`n` roughly equal chunks, where :math:`n` is the number of worker
   processes. Each worker gets one fixed chunk.

2. **Independent Processing**: Each worker process processes its
   assigned chunk independently, finding cycles and computing periods
   without communication with other workers.

3. **Result Merging**: After all workers complete, results are merged
   with automatic deduplication of sequences that may have been found
   by multiple workers.

**Dynamic Load Balancing (Shared Task Queue)**:

1. **Task Queue Creation**: States are divided into small batches
   (typically 200 states) and placed in a shared queue accessible by
   all workers.

2. **Dynamic Work Distribution**: Workers continuously pull batches
   from the queue:
   
   - When a worker finishes a batch, it immediately pulls the next
     available batch
   - Faster workers naturally take on more work
   - This provides automatic load balancing, reducing imbalance by
     2-4x for multi-cycle LFSRs

3. **Result Merging**: After all workers complete, results are merged
   with automatic deduplication (same as static mode).

**When to Use Each Mode**:

- **Static Mode**: Best for LFSRs with few cycles (2-4 cycles) or when
  cycles are evenly distributed
- **Dynamic Mode**: Best for LFSRs with many cycles (8+ cycles),
  providing significantly better load balancing

**Algorithm**:

.. math::

   \begin{aligned}
   \text{Partition}(V, n) &: \text{Divide state space } V \text{ into } n \text{ chunks} \\
   \text{ProcessChunk}(C_i) &: \text{Process chunk } C_i \text{ independently} \\
   \text{Merge}(R_1, \ldots, R_n) &: \text{Combine and deduplicate results}
   \end{aligned}

**State Space Partitioning**:

The partitioning function divides the state space into chunks:

.. math::

   \text{chunk\_size} = \left\lceil \frac{|V|}{n} \right\rceil

   C_i = \{v_j : i \cdot \text{chunk\_size} \leq j < (i+1) \cdot \text{chunk\_size}\}

Each state is converted to a tuple (for pickling/serialization) since
SageMath vectors are not directly pickleable for inter-process communication.

**Worker Processing**:

Each worker process:

1. Reconstructs SageMath objects from serialized data (tuples)
2. Rebuilds the state update matrix from coefficients extracted from
   the **last column** of the companion matrix (not the last row). The
   companion matrix structure stores coefficients :math:`c_0, c_1,
   \ldots, c_{d-1}` in column :math:`d-1` at positions :math:`(i,
   d-1)` for :math:`i = 0, \ldots, d-1`.
3. Processes each state in its chunk:

   - Reconstructs state vector from tuple
   - **Period Computation**: Uses Floyd's algorithm
     (``_find_period_floyd``) to compute the period. Enumeration-based
     methods are avoided due to matrix multiplication loops that hang
     in multiprocessing context.
   - **Sequence Computation for Deduplication**: For periods
     :math:`\leq 100`, computes the full sequence using direct
     enumeration to enable proper deduplication. For larger periods,
     uses simplified deduplication based on
     :math:`(\text{start\_state}, \text{period})`.
   - Marks states in cycle as visited (local to worker)
   - Stores sequence information with ``period_only`` flag
4. Returns results: sequences, periods, max period, errors

**Critical Implementation Details**:

* **Algorithm Selection**: Floyd's algorithm is **required** for
  parallel processing because enumeration-based methods (which use
  matrix multiplication in tight loops) hang after approximately 12
  iterations in multiprocessing context. This is a known
  SageMath/multiprocessing interaction issue.

* **Period-Only Mode**: Parallel processing **requires** period-only
  mode (``--period-only`` flag). Full sequence mode hangs due to the
  enumeration loop issue. The tool automatically forces period-only
  mode when parallel processing is enabled, displaying a warning to
  the user.

* **Matrix Coefficient Extraction**: The companion matrix stores
  coefficients in the **last column** (column :math:`d-1`), not the
  last row. Extraction must use: :math:`c_i = C[i, d-1]` for :math:`i
  = 0, \ldots, d-1`. This is critical for correct matrix
  reconstruction in worker processes.

**Result Merging and Deduplication**:

Since multiple workers may process states from the same cycle, results
must be deduplicated:

- **For small periods** (:math:`\leq 100`): Workers compute the full
  sequence (even in period-only mode) for deduplication purposes. The
  merge function uses the sorted tuple of all state tuples in the
  cycle as the deduplication key.  This ensures accurate deduplication
  since cycles are identical regardless of starting point.

- **For large periods** (:math:`> 100`): To avoid hangs from matrix
  multiplication loops, workers use simplified deduplication based on
  :math:`(\text{start\_state}, \text{period})`.  This may result in
  some false duplicates not being caught, but is an acceptable
  trade-off to avoid hangs.

- **Period-Only Flag**: The merge function respects the
  ``period_only`` flag in sequence information. Even though full
  sequences may be computed for deduplication, they are not stored in
  the final output when ``period_only=True``.

The merge function:

1. Collects all sequences from all workers
2. Creates canonical representations of cycles:
   - Small periods: Sorted tuple of all state tuples
   - Large periods: :math:`(\text{start\_state}, \text{period})` tuple
3. Deduplicates based on cycle identity
4. Reconstructs SageMath objects (only if ``period_only=False``)
5. Assigns sequential sequence numbers
6. Verifies correctness: :math:`\sum \text{periods} = q^d`

**Performance Characteristics**:

* **Theoretical Speedup**: Up to :math:`n`-fold on :math:`n` cores (linear
  scaling for independent work)
* **Practical Speedup**: 4-8× on typical multi-core systems (due to overhead)
* **Overhead**: Process creation, IPC, result merging
* **Best Case**: Large state spaces (> 10,000 states) with many CPU cores

**Complexity Analysis**:

* **Time**: :math:`O(q^d / n)` per worker (theoretical),
  :math:`O(q^d)` total (amortized)
* **Space**: :math:`O(q^d)` total (same as sequential, distributed
  across workers)
* **Communication**: Minimal (only at start and end, no inter-worker
  communication)

**Automatic Selection**:

The tool automatically enables parallel processing when:

.. math::

   |V| > 10,000 \text{ and } n_{\text{cores}} \geq 2

This threshold balances the overhead of multiprocessing against the
benefits of parallelization.

**Graceful Degradation**:

If parallel processing fails or times out, the tool automatically
falls back to sequential processing. This ensures:

1. **Reliability**: Tool always completes successfully
2. **Correctness**: Results are identical regardless of processing method
3. **User Experience**: No manual intervention required

**Known Limitations**:

* **Full Sequence Mode Hang**: Full sequence mode (without
  ``--period-only``) causes workers to hang during matrix
  multiplication loops in enumeration-based methods.  This is a
  fundamental SageMath/multiprocessing interaction
  issue. **Workaround**: Parallel processing automatically forces
  period-only mode, displaying a warning.  Use ``--no-parallel`` for
  full sequence mode.

* **Algorithm Restriction**: Only Floyd's algorithm is used in
  parallel workers, regardless of the ``--algorithm``
  flag. Enumeration and Brent's algorithms are not used due to the
  matrix multiplication hang issue.

* **Deduplication for Large Periods**: For periods :math:`> 100`,
  deduplication uses simplified keys that may not catch all
  duplicates. This is an acceptable trade-off to avoid computing full
  sequences (which would hang).

* **SageMath Compatibility**: Some SageMath/multiprocessing
  configurations may cause workers to hang. The timeout mechanism
  detects this and falls back to sequential processing.

* **Small State Spaces**: Overhead of multiprocessing may outweigh
  benefits for small LFSRs (< 10,000 states).

* **Memory**: Each worker maintains its own copy of SageMath objects,
  but total memory usage is similar to sequential processing.

* **Matrix Coefficient Extraction**: Critical that coefficients are
  extracted from the **last column** of the companion matrix, not the
  last row. Incorrect extraction leads to wrong matrix reconstruction
  and incorrect period computations.

**Performance Results**:

After optimization (lazy partitioning), parallel processing achieves
excellent speedup for medium-sized LFSRs:

* **7-bit LFSR (128 states)**: 6.37x - 9.89x speedup
* **Best configuration**: 1-2 workers for medium LFSRs
* **Efficiency**: 159% - 989% (overhead reduction from optimization)
* **Overhead**: Negative in some cases (optimization improved
  performance)

**Optimization Implemented**:

The main bottleneck (state space partitioning, 60% of time) was
optimized using lazy iteration:

* **Before**: Materialized all states upfront, then partitioned
* **After**: Lazy iteration with on-the-fly conversion to tuples
* **Result**: 6-10x speedup improvement for medium LFSRs

**Future Improvements**:

* Dynamic load balancing (instead of static partitioning)
* Shared memory for visited set (with proper locking)
* Progress tracking across workers
* Alternative parallelization approaches (threading,
  concurrent.futures)
* Further reduce process overhead (reuse workers, cache
  reconstruction)

Period Distribution Statistics
-------------------------------

The tool computes comprehensive statistical analysis of period
distribution across all sequences in an LFSR. This provides insights
into how periods are distributed and how they compare with theoretical
expectations.

**Distribution Metrics**:

The tool computes:

- **Mean Period**: Average period across all sequences
- **Median Period**: Middle value when periods are sorted
- **Variance**: Measure of how spread out periods are
- **Standard Deviation**: Square root of variance
- **Minimum/Maximum Period**: Smallest and largest periods
- **Period Frequency**: Histogram showing how many sequences have each
  period value

**Theoretical Bounds**:

For an LFSR of degree :math:`d` over :math:`\mathbb{F}_q`:

- **Maximum Theoretical Period**: :math:`q^d - 1` (all states except
  zero)
- **State Space Size**: :math:`q^d` (total number of possible states)

**Primitive Polynomial Analysis**:

When the characteristic polynomial is primitive:

- All non-zero states should have period :math:`q^d - 1`
- The period distribution should show all sequences with the maximum
  period
- This is verified automatically in the comparison section

**Period Diversity**:

The period diversity metric is defined as:

.. math::

   \text{Diversity} = \frac{\text{Unique Periods}}{\text{Total Sequences}}

A diversity of 1.0 means all sequences have different periods, while
lower values indicate more sequences share the same period.

**Comparison with Theoretical Bounds**:

The tool compares:

- Whether the maximum observed period equals the theoretical maximum
- The ratio of maximum period to theoretical maximum
- For primitive polynomials: whether all periods are maximum

This analysis helps validate theoretical predictions and understand
the structure of LFSR period distributions.

Polynomial Factorization and Factor Orders
------------------------------------------

Factorization
~~~~~~~~~~~~~

The characteristic polynomial can be factored over :math:`\mathbb{F}_q`:

.. math::

   P(t) = \prod_{i=1}^r f_i(t)^{e_i}

where :math:`f_i(t)` are irreducible polynomials and :math:`e_i` are
their multiplicities.

.. prf:example:: Polynomial Factorization
   :label: ex-poly-factorization

   Over :math:`\mathbb{F}_2`:

   .. math::

      t^4 + t^3 + t + 1 = (t+1)(t^3 + t + 1)

Factor Orders
~~~~~~~~~~~~~

Each factor :math:`f_i(t)` has its own order :math:`n_i` (smallest
:math:`n` such that :math:`t^n \equiv 1 \pmod{f_i(t)}`).

.. prf:theorem:: Order from Irreducible Factors
   :label: theorem-order-from-factors

   The order of :math:`P(t)` is the least common
   multiple (LCM) of the orders of its irreducible factors (with
   appropriate handling of multiplicities).

.. prf:proof::

   Proof sketch:

   If :math:`P(t) = f_1(t)^{e_1} f_2(t)^{e_2} \cdots f_r(t)^{e_r}`, and
   each :math:`f_i(t)` has order :math:`n_i`, then:

   * :math:`t^{n_i} \equiv 1 \pmod{f_i(t)}`
   * For :math:`t^n \equiv 1 \pmod{P(t)}`, we need :math:`t^n \equiv 1
     \pmod{f_i(t)^{e_i}}` for all :math:`i`
   * This requires :math:`n` to be a multiple of :math:`n_i` (and
     possibly :math:`p \cdot n_i` if :math:`e_i > 1` and :math:`p`
     divides :math:`n_i`)
   * Therefore, :math:`n = \text{lcm}(n_1, n_2, \ldots, n_r)` (with
     appropriate adjustments)

.. prf:example:: Order from Factors
   :label: ex-order-from-factors

   For :math:`P(t) = (t+1)(t^3 + t + 1)`:

   * Order of :math:`t+1`: 1 (since :math:`t \equiv -1 \equiv 1
     \pmod{t+1}` in :math:`\mathbb{F}_2`)
   * Order of :math:`t^3 + t + 1`: 7
   * Order of :math:`P(t)`: :math:`\text{lcm}(1, 7) = 7`

However, if the polynomial is not square-free, the calculation is more
complex.

Berlekamp-Massey Algorithm
---------------------------

Problem Statement
~~~~~~~~~~~~~~~~~

Given a sequence :math:`s_0, s_1, s_2, \ldots, s_{n-1}` over
:math:`\mathbb{F}_q`, find the **shortest** LFSR that can generate
this sequence.

Algorithm Description
~~~~~~~~~~~~~~~~~~~~~

The Berlekamp-Massey algorithm is an iterative algorithm that
constructs the minimal LFSR:

1. **Initialize**: Start with trivial LFSR of length 0
2. **Iterate**: For each new sequence element:
   
   a. Check if current LFSR correctly predicts the next element
   b. If correct, continue
   c. If incorrect (discrepancy found):

      * Update LFSR to correct the discrepancy
      * May need to increase LFSR length
3. **Output**: Minimal LFSR (coefficients and length)

Mathematical Foundation
~~~~~~~~~~~~~~~~~~~~~~~~

The algorithm maintains:

* **Current LFSR**: Represented by polynomial :math:`C(x) = 1 + c_1
  x + c_2 x^2 + \cdots + c_L x^L`
* **Discrepancy**: Difference between predicted and actual sequence
  value
* **Previous LFSR**: For backtracking when length increases

**Key Insight**: The minimal LFSR length equals the
  **linear complexity** of the sequence.

.. prf:theorem:: Berlekamp-Massey Correctness
   :label: theorem-berlekamp-massey

   The Berlekamp-Massey algorithm finds the unique minimal
   LFSR in :math:`O(n^2)` time.

.. prf:example:: Berlekamp-Massey Algorithm
   :label: ex-berlekamp-massey

   Sequence :math:`[1, 0, 1, 1, 0, 1, 0, 0, 1]` over
   :math:`\mathbb{F}_2`:

   * Initial: :math:`C(x) = 1`, length :math:`L = 0`
   * Process :math:`s_0 = 1`: No discrepancy, continue
   * Process :math:`s_1 = 0`: Discrepancy, update :math:`C(x) = 1 + x`, :math:`L = 1`
   * Process :math:`s_2 = 1`: Check prediction...
   * Continue until minimal LFSR found

   The algorithm will find that this sequence can be generated by an LFSR
   of length 4. The exact coefficients depend on the sequence and initial
   state.

Linear Complexity
-----------------

Definition
~~~~~~~~~~

The **linear complexity** :math:`L(s)` of a sequence :math:`s = s_0,
s_1, s_2, \ldots` is the length of the shortest LFSR that can generate
it.

Properties
~~~~~~~~~~

1. **Bounded**: For a sequence of length :math:`n`, :math:`0 \leq L(s)
   \leq n`
2. **Uniqueness**: The minimal LFSR is unique (up to initial state)
3. **Random Sequences**: A truly random sequence has linear complexity
   approximately :math:`n/2`

Linear Complexity Profile
~~~~~~~~~~~~~~~~~~~~~~~~~

The **linear complexity profile** is the sequence :math:`L_1, L_2,
\ldots, L_n` where :math:`L_i` is the linear complexity of the first
:math:`i` elements.

**Properties**:

* :math:`L_i \leq L_{i+1}` (complexity can only increase)
* :math:`L_{i+1} - L_i \leq 1` (complexity increases by at most 1 per
  step)
* If :math:`L_{i+1} > L_i`, then :math:`L_{i+1} = \max(L_i, i+1 - L_i)`

**Application**: Used in cryptanalysis to detect non-randomness in
 sequences.

Statistical Tests
-----------------

Frequency Test
~~~~~~~~~~~~~~

Tests whether the distribution of symbols in the sequence matches the
expected uniform distribution.

**Test Statistic**: For sequence of length :math:`n` over
 :math:`\mathbb{F}_q`:

.. math::

   \chi^2 = \sum_{a \in \mathbb{F}_q} \frac{(n_a - n/q)^2}{n/q}

where :math:`n_a` is the count of symbol :math:`a`.

**Expected**: :math:`\chi^2 \sim \chi^2(q-1)` under the null
 hypothesis of uniformity.

Runs Test
~~~~~~~~~

Tests for patterns and clustering in the sequence.

A **run** is a maximal subsequence of consecutive identical symbols.

**Test**: Count the number of runs and compare to expected
 distribution for a random sequence.

Autocorrelation
~~~~~~~~~~~~~~~

Measures the correlation between a sequence and shifted versions of
itself.

.. prf:definition:: Autocorrelation Function
   :label: def-autocorrelation

   For lag :math:`k`:

   .. math::

      R(k) = \frac{1}{n} \sum_{i=0}^{n-k-1} (-1)^{s_i + s_{i+k}}

   For binary sequences, this becomes:

   .. math::

      R(k) = \frac{1}{n} \sum_{i=0}^{n-k-1} (1 - 2(s_i \oplus s_{i+k}))

**Properties**:

* :math:`R(0) = 1` (perfect autocorrelation at lag 0)
* For random sequences, :math:`R(k) \approx 0` for :math:`k \neq 0`
* LFSR sequences have specific autocorrelation properties

Periodicity Test
~~~~~~~~~~~~~~~~

Detects periodic patterns in the sequence.

**Method**: Check if :math:`s_i = s_{i+k}` for various lags :math:`k`.

**Application**: Can reveal if sequence is periodic (which LFSR
 sequences are, by definition).

Comprehensive Example
---------------------

Let's work through a complete example: LFSR with coefficients
:math:`[1, 1, 0, 0]` over :math:`\mathbb{F}_2` (primitive polynomial
case).

Step 1: State Update Matrix
~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. math::

   C = \begin{pmatrix}
   0 & 0 & 0 & 1 \\
   1 & 0 & 0 & 1 \\
   0 & 1 & 0 & 0 \\
   0 & 0 & 1 & 0
   \end{pmatrix}

The coefficients :math:`c_0 = 1, c_1 = 1, c_2 = 0, c_3 = 0` are stored
in the last column at positions :math:`(0,3), (1,3), (2,3), (3,3)`
respectively.

Step 2: Characteristic Polynomial
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. math::

   P(t) = \det(tI - C) = t^4 + t + 1

This is a **primitive polynomial** over :math:`\mathbb{F}_2`.

Step 3: Polynomial Order
~~~~~~~~~~~~~~~~~~~~~~~~~

We verify that :math:`t^{15} \equiv 1 \pmod{P(t)}`:

* :math:`P(t) = t^4 + t + 1` (irreducible and primitive)
* Computing :math:`t^n \bmod P(t)` for :math:`n = 1, 2, \ldots, 15`
* Find :math:`t^{15} \equiv 1`, so order is 15
* Since :math:`15 = 2^4 - 1`, this confirms :math:`P(t)` is primitive

Step 4: Matrix Order
~~~~~~~~~~~~~~~~~~~~

Since the polynomial order equals the matrix order, :math:`C^{15} = I`.

Step 5: Sequence Analysis
~~~~~~~~~~~~~~~~~~~~~~~~~

Starting from state :math:`S_0 = (s_0, s_1, s_2, s_3) = (1, 0, 0, 0)`:

.. math::

   \begin{aligned}
   S_0 &= (1, 0, 0, 0) \\
   S_1 &= (0, 0, 0, 1) \\
   S_2 &= (0, 0, 1, 0) \\
   S_3 &= (0, 1, 0, 0) \\
   S_4 &= (1, 0, 0, 1) \\
   S_5 &= (0, 0, 1, 1) \\
   &\vdots \\
   S_{15} &= (1, 0, 0, 0) = S_0
   \end{aligned}

The sequence has period 15, which equals the matrix order. Since the
polynomial is primitive, all non-zero states have the maximum period
of 15.

Step 6: Factorization
~~~~~~~~~~~~~~~~~~~~~

Since :math:`P(t) = t^4 + t + 1` is **irreducible** (and primitive)
over :math:`\mathbb{F}_2`, it does not factor into lower-degree
polynomials.  The factorization is trivial: :math:`P(t) = t^4 + t + 1`
itself.

For a reducible polynomial example, consider :math:`Q(t) = t^4 + t^3 +
t + 1`:

.. math::

   Q(t) = t^4 + t^3 + t + 1 = (t+1)^2 (t^2 + t + 1)

* Factor :math:`(t+1)` has order 1
* Factor :math:`(t^2 + t + 1)` has order 3
* Order of :math:`Q(t)`: :math:`\text{lcm}(1, 3) = 3` (with
  appropriate handling of the square factor)

**Note**: The order calculation for polynomials with repeated factors
requires considering the multiplicities. For square-free polynomials,
the order is the LCM of the orders of the irreducible factors.

Theoretical Results and Theorems
---------------------------------

.. prf:theorem:: Maximum Period
   :label: theorem-maximum-period

   For an LFSR of degree :math:`d` over
   :math:`\mathbb{F}_q`, the maximum possible period is :math:`q^d - 1`.

.. prf:proof::

   * There are :math:`q^d` possible states
   * The all-zero state :math:`(0, 0, \ldots, 0)` is fixed (period 1)
   * All other states form cycles
   * Maximum cycle length is :math:`q^d - 1`

**Achievability**: The maximum period is achieved if and only if the
 characteristic polynomial is **primitive**.

.. prf:definition:: Primitive Polynomial
   :label: def-primitive-polynomial

   A polynomial :math:`P(t)` of degree :math:`d` over
   :math:`\mathbb{F}_q` is **primitive** if:

   1. :math:`P(t)` is irreducible
   2. The order of :math:`P(t)` is :math:`q^d - 1`

.. prf:theorem:: Primitive Polynomial Period
   :label: theorem-primitive-period

   If :math:`P(t)` is primitive, then the LFSR has maximum
   period :math:`q^d - 1`, and the generated sequence has excellent
   statistical properties.

.. prf:example:: Primitive Polynomial Example
   :label: ex-primitive-polynomial

   Over :math:`\mathbb{F}_2`, the polynomial :math:`t^4 +
   t + 1` is primitive and has order 15, giving maximum period.

**Implementation**: The tool automatically detects primitive
 polynomials and displays a ``[PRIMITIVE]`` indicator in the
 characteristic polynomial output. This can be explicitly checked
 using the ``--check-primitive`` command-line flag. The detection uses
 SageMath's built-in ``is_primitive()`` method when available, or
 falls back to checking irreducibility and verifying that the
 polynomial order equals :math:`q^d - 1`.

.. prf:theorem:: All Periods Divide Matrix Order
   :label: theorem-all-periods-divide

   All sequence periods divide the matrix order.

.. prf:proof::

   If :math:`C^n = I` and sequence has period :math:`k`, then:

   .. math::

      S_k = S_0 \Rightarrow S_0 \cdot C^k = S_0

   Since :math:`C^n = I`, we have :math:`S_0 = S_0 \cdot C^n`. If
   :math:`k` doesn't divide :math:`n`, we can write :math:`n = qk + r`
   with :math:`0 < r < k`, leading to a contradiction.

Therefore, :math:`k \mid n`.

.. prf:theorem:: Linear Complexity Lower Bound
   :label: theorem-linear-complexity-lower-bound

   For a sequence of length :math:`n` over
   :math:`\mathbb{F}_q`, the linear complexity :math:`L` satisfies:

.. math::

   L \geq \frac{n}{2}

with high probability for random sequences.

**Implication**: Sequences with low linear complexity are
 cryptographically weak.

Applications in Cryptography
----------------------------

Stream Ciphers
~~~~~~~~~~~~~~

LFSRs are used in stream ciphers (e.g., A5/1, A5/2 in GSM):

* **Advantages**: Fast, simple hardware implementation
* **Disadvantages**: Linear structure makes them vulnerable to attacks
* **Solution**: Combine multiple LFSRs with nonlinear functions

Key Generation
~~~~~~~~~~~~~~

LFSRs can generate pseudorandom sequences for key material, but
require:

* Primitive polynomials for maximum period
* Nonlinear combination for security
* Proper initialization (avoid all-zero state)

Cryptanalysis
~~~~~~~~~~~~~

Attacks on LFSR-based systems:

* **Berlekamp-Massey Attack**: Recover LFSR from known plaintext
* **Correlation Attack**: Exploit correlations in combined LFSRs
* **Fast Correlation Attack**: Use iterative decoding for efficient
  state recovery
* **Distinguishing Attack**: Detect if keystream is distinguishable
  from random
* **Algebraic Attack**: Solve systems of equations
* **Time-Memory Trade-Off (TMTO)**: Precompute states for faster
  attacks

.. _performance-analysis:

Performance Analysis and Algorithm Comparison
-----------------------------------------------

This section provides detailed analysis of cycle detection algorithm
performance based on empirical testing and theoretical analysis.

Operation Count Analysis
~~~~~~~~~~~~~~~~~~~~~~~~~

**Floyd's Algorithm Operation Count**:

For a period :math:`\lambda`, Floyd's algorithm performs:

* **Phase 1** (Find Meeting Point):
  
  * Tortoise moves: :math:`\sim \lambda/2` steps (on average for LFSRs)
  * Hare moves: :math:`2 \times \lambda/2 = \lambda` steps (double speed)
  * Total Phase 1 operations: :math:`3 \times \lambda/2 = 1.5\lambda`
    matrix multiplications

* **Phase 2** (Find Period):
  
  * Hare moves: :math:`\lambda` steps
  * Total Phase 2 operations: :math:`\lambda` matrix multiplications

* **Total Floyd Operations**: :math:`\sim 1.5\lambda + \lambda =
  2.5\lambda` operations

**Enumeration Algorithm Operation Count**:

* **Total Operations**: :math:`\lambda` matrix multiplications (one
  per state in cycle)

**Comparison**:

* Floyd performs approximately **2.5× more operations** than
  enumeration
* Actual measured ratio: **~3.83×** (due to implementation details and
  meeting point location)
* For period 24: Floyd = 92 operations, Enumeration = 24 operations

Time Performance Analysis
~~~~~~~~~~~~~~~~~~~~~~~~~

**Empirical Results** (Period-Only Mode):

For typical LFSR periods (8-24):

* **Floyd**: ~2.0 ms (92 operations for period 24)
* **Enumeration**: ~0.5 ms (24 operations for period 24)
* **Speedup**: Enumeration is **3-5× faster**
* **Time per Operation**: Similar (~0.022 ms for both algorithms)

**Why Floyd is Slower**:

1. **More Operations**: Floyd does ~4× more matrix multiplications
2. **Overhead Dominates**: Phase 1 + Phase 2 overhead outweighs
   benefits for small periods
3. **No Compensating Advantage**: Both algorithms are O(1) space, so
   Floyd's theoretical advantage doesn't apply

**Scaling Behavior**:

* For periods < 100: Enumeration is consistently faster
* For periods 100-1000: Enumeration remains faster (overhead still
  dominates)
* For periods > 1000: Needs testing, but overhead likely still
  dominates for typical LFSRs

Space Complexity Analysis
~~~~~~~~~~~~~~~~~~~~~~~~~

**Period-Only Mode** (``--period-only``):

Both algorithms achieve **true O(1) space**:

* **Floyd**: ~1.60 KB (constant, verified across iterations and period
  sizes)
* **Enumeration**: ~1.44 KB (constant, verified across iterations and
  period sizes)
* **Memory Independence**: Memory usage is constant regardless of
  period size ✓

**Full Sequence Mode**:

Both algorithms use **O(λ) space**:

* Must store full sequence for output
* Floyd's O(1) space advantage doesn't apply
* Enumeration is simpler and faster

**Verification**:

Memory profiling shows:

* Coefficient of variation < 0.1% for both algorithms in period-only
  mode
* Memory usage constant across period range 8-24
* True O(1) space confirmed ✓

Algorithm Selection Guidelines
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Use Enumeration When**:

* Computing full sequences (default, faster)
* Computing periods only (default, faster)
* Period < 1000 (typical case)
* Simplicity and speed are priorities

**Use Floyd When**:

* Educational/verification purposes
* Very large periods (> 10,000) - needs verification
* Want to verify results with different algorithm
* Exploring algorithm properties

**Default Behavior**:

* **Full Mode**: Enumeration (faster, simpler)
* **Period-Only Mode**: Enumeration (faster, both are O(1) space)
* **Auto Mode**: Selects enumeration for full mode, Floyd for
  period-only (but enumeration is still recommended)

Performance Profiling Tools
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The tool provides several profiling scripts:

* ``scripts/performance_profile.py``: Comprehensive algorithm
  comparison
  
  * Use ``--period-only`` flag for period-only mode analysis
  * Measures time, memory, and operation counts
  * Verifies space complexity claims

* ``scripts/detailed_performance_analysis.py``: Phase-by-phase
  analysis
  
  * Breaks down Floyd into Phase 1 and Phase 2
  * Analyzes operation counts in detail
  * Tests memory patterns

* ``scripts/analyze_floyd_overhead.py``: Overhead analysis
  
  * Explains why Floyd does more operations
  * Compares with different period sizes
  * Finds break-even points

**Example Usage**:

.. code-block:: bash

   # Compare algorithms in period-only mode
   python3 scripts/performance_profile.py input.csv 2 --period-only -n 10
   
   # Detailed phase analysis
   python3 scripts/detailed_performance_analysis.py input.csv 2
   
   # Overhead analysis
   python3 scripts/analyze_floyd_overhead.py input.csv 2

Summary and Recommendations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Key Findings**:

1. ✅ **Floyd is correctly implemented** - algorithm works as designed
2. ✅ **O(1) space achieved** - both algorithms achieve true O(1)
   space in period-only mode
3. ❌ **Floyd is slower** - does ~4× more operations, making it 3-5×
   slower
4. ❌ **No practical advantage** - enumeration is better for typical
   LFSR periods

**Recommendations**:

1. **Default to Enumeration**: Simpler, faster, uses less memory
2. **Keep Floyd as Option**: For educational and verification purposes
3. **Document Trade-offs**: Clearly explain performance
   characteristics
4. **Use Period-Only Mode**: When only periods are needed, both
   achieve O(1) space

**Conclusion**:

Floyd's cycle detection algorithm is a classic algorithm with
theoretical elegance, but for practical LFSR analysis, enumeration is
the better choice. Floyd's O(1) space advantage is achieved by
enumeration in period-only mode, and enumeration's simplicity and
speed make it superior for typical use cases.

References and Further Reading
-------------------------------

Classical Texts
~~~~~~~~~~~~~~~

* **Golomb, S. W.** (1967). *Shift Register
  Sequences*. Holden-Day. The foundational text on LFSRs and
  pseudorandom sequences.

* **Lidl, R. & Niederreiter, H.** (1997). *Finite Fields*. Cambridge
  University Press. Comprehensive treatment of finite field theory.

Modern References
~~~~~~~~~~~~~~~~~

* **Menezes, A. J., van Oorschot, P. C., & Vanstone, S. A.**
  (1996). *Handbook of Applied Cryptography*. CRC Press. Chapter 6
  covers LFSRs and stream ciphers.

* **Rueppel, R. A.** (1986). *Analysis and Design of Stream
  Ciphers*. Springer. Detailed analysis of LFSR-based cryptographic
  systems.

Online Resources
~~~~~~~~~~~~~~~~

* **Tanja Lange's Cryptology Course**:
  https://www.hyperelliptic.org/tanja/teaching/CS22/
  
   * Exercise 2 motivates this tool's development
   * Excellent introduction to LFSRs and their analysis

* **SageMath Documentation**: https://doc.sagemath.org/
  
   * Finite field operations
   * Polynomial manipulation
   * Matrix computations over finite fields

Mathematical Software
~~~~~~~~~~~~~~~~~~~~~

* **SageMath**: Open-source mathematics software system used by this
  tool
* **Magma**: Commercial computer algebra system with excellent finite
  field support
* **GAP**: System for computational discrete algebra

---

**Note**: This mathematical background provides the theoretical
 foundation for understanding LFSR analysis. For practical usage
 examples, see the :doc:`examples` section. For API documentation, see
 the :doc:`api/index` section.
