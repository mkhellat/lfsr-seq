#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Optimization utilities for LFSR analysis.

This module provides caching and optimization utilities to improve performance
for repeated analyses and structured problems.
"""

import hashlib
import json
import os
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from lfsr.polynomial import compute_period_via_factorization, detect_mathematical_shortcuts


class ResultCache:
    """
    Cache for LFSR analysis results.
    
    This class provides both in-memory and persistent (file-based) caching
    to avoid redundant computations. Cached results are keyed by a hash of
    the LFSR configuration (coefficients + field order).
    
    **Key Terminology**:
    
    - **Cache**: A storage mechanism for computed results to avoid redundant
      calculations. Caching is a fundamental optimization technique.
    
    - **Cache Key**: A unique identifier for a computation. In our case, it's
      a hash of the LFSR configuration (coefficients and field order).
    
    - **Cache Hit**: When a requested result is found in the cache, avoiding
      the need to recompute it.
    
    - **Cache Miss**: When a requested result is not in the cache, requiring
      computation.
    
    - **Cache Invalidation**: Removing stale or incorrect entries from the cache.
      This is important when algorithms change or bugs are fixed.
    
    - **Persistent Cache**: A cache that survives between program runs, stored
      on disk (typically as a JSON file). This allows results to be reused
      across different sessions.
    
    - **In-Memory Cache**: A cache stored in RAM that is fast but lost when
      the program exits. Used for temporary results within a single session.
    
    **Cache Strategy**:
    
    The cache uses a two-level approach:
    1. **In-Memory Cache**: Fast dictionary-based storage for current session
    2. **Persistent Cache**: File-based storage for cross-session reuse
    
    **Cache Key Generation**:
    
    Cache keys are generated by hashing the LFSR configuration:
    - Coefficients (normalized)
    - Field order
    - Analysis type (period, polynomial, etc.)
    
    This ensures that identical LFSR configurations produce the same key.
    
    **Example Usage**:
    
    .. code-block:: python
    
       from lfsr.optimization import ResultCache
       
       # Create cache with persistent storage
       cache = ResultCache(cache_file="~/.lfsr-seq/cache.json")
       
       # Check if result is cached
       key = cache.generate_key([1, 0, 0, 1], 2)
       if key in cache:
           result = cache.get(key)
       else:
           result = compute_period([1, 0, 0, 1], 2)
           cache.set(key, result)
    
    Attributes:
        cache_file: Optional path to persistent cache file
        in_memory_cache: Dictionary for in-memory caching
        cache_stats: Statistics about cache usage
    """
    
    def __init__(self, cache_file: Optional[str] = None):
        """
        Initialize cache with optional persistent storage.
        
        Args:
            cache_file: Optional path to JSON file for persistent caching.
                        If None, only in-memory caching is used.
                        If path contains '~', it will be expanded to home directory.
        """
        self.cache_file = cache_file
        if self.cache_file:
            # Expand user home directory
            self.cache_file = os.path.expanduser(self.cache_file)
            # Create directory if it doesn't exist
            cache_dir = os.path.dirname(self.cache_file)
            if cache_dir:
                os.makedirs(cache_dir, exist_ok=True)
        
        self.in_memory_cache: Dict[str, Any] = {}
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'sets': 0,
            'loads': 0
        }
        
        # Load persistent cache if it exists
        if self.cache_file and os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r') as f:
                    persistent_cache = json.load(f)
                    self.in_memory_cache.update(persistent_cache)
                    self.cache_stats['loads'] = len(persistent_cache)
            except (json.JSONDecodeError, IOError, OSError):
                # If cache file is corrupted, start fresh
                self.in_memory_cache = {}
    
    def generate_key(
        self,
        coefficients: List[int],
        field_order: int,
        analysis_type: str = "period"
    ) -> str:
        """
        Generate a cache key from LFSR configuration.
        
        The key is a hash of the normalized configuration, ensuring that
        identical LFSRs produce identical keys.
        
        Args:
            coefficients: List of feedback polynomial coefficients
            field_order: The field order
            analysis_type: Type of analysis (e.g., "period", "polynomial")
        
        Returns:
            Hexadecimal hash string serving as cache key
        """
        # Normalize coefficients (remove trailing zeros for consistency)
        normalized_coeffs = coefficients[:]
        while normalized_coeffs and normalized_coeffs[-1] == 0:
            normalized_coeffs.pop()
        
        # Create hashable representation
        config_str = json.dumps({
            'coefficients': normalized_coeffs,
            'field_order': field_order,
            'analysis_type': analysis_type
        }, sort_keys=True)
        
        # Generate hash
        hash_obj = hashlib.sha256(config_str.encode('utf-8'))
        return hash_obj.hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """
        Retrieve cached result.
        
        Checks both in-memory and persistent cache. Updates cache statistics.
        
        Args:
            key: Cache key generated by generate_key()
        
        Returns:
            Cached result if found, None otherwise
        """
        if key in self.in_memory_cache:
            self.cache_stats['hits'] += 1
            return self.in_memory_cache[key]
        
        self.cache_stats['misses'] += 1
        return None
    
    def set(self, key: str, value: Any) -> None:
        """
        Store result in cache.
        
        Stores in both in-memory cache and persistent cache (if configured).
        Updates cache statistics.
        
        Args:
            key: Cache key generated by generate_key()
            value: Result to cache (must be JSON-serializable)
        """
        self.in_memory_cache[key] = value
        self.cache_stats['sets'] += 1
        
        # Save to persistent cache if configured
        if self.cache_file:
            try:
                with open(self.cache_file, 'w') as f:
                    json.dump(self.in_memory_cache, f, indent=2)
            except (IOError, OSError):
                # If write fails, continue with in-memory cache only
                pass
    
    def clear(self) -> None:
        """
        Clear all cached results.
        
        Clears both in-memory and persistent cache. Resets statistics.
        """
        self.in_memory_cache.clear()
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'sets': 0,
            'loads': 0
        }
        
        # Remove persistent cache file if it exists
        if self.cache_file and os.path.exists(self.cache_file):
            try:
                os.remove(self.cache_file)
            except (IOError, OSError):
                pass
    
    def get_stats(self) -> Dict[str, int]:
        """
        Get cache statistics.
        
        Returns:
            Dictionary with cache statistics:
            - 'hits': Number of cache hits
            - 'misses': Number of cache misses
            - 'sets': Number of values stored
            - 'loads': Number of values loaded from persistent cache
            - 'hit_rate': Cache hit rate (hits / (hits + misses))
        """
        stats = self.cache_stats.copy()
        total_requests = stats['hits'] + stats['misses']
        if total_requests > 0:
            stats['hit_rate'] = stats['hits'] / total_requests
        else:
            stats['hit_rate'] = 0.0
        return stats
    
    def __contains__(self, key: str) -> bool:
        """Check if key exists in cache."""
        return key in self.in_memory_cache
    
    def __len__(self) -> int:
        """Get number of cached entries."""
        return len(self.in_memory_cache)


# Global cache instance (can be configured)
_global_cache: Optional[ResultCache] = None


def get_global_cache() -> ResultCache:
    """
    Get or create global cache instance.
    
    Returns:
        Global ResultCache instance
    """
    global _global_cache
    if _global_cache is None:
        # Default cache location: ~/.lfsr-seq/cache.json
        cache_file = os.path.expanduser("~/.lfsr-seq/cache.json")
        _global_cache = ResultCache(cache_file=cache_file)
    return _global_cache


def set_global_cache(cache: ResultCache) -> None:
    """
    Set global cache instance.
    
    Args:
        cache: ResultCache instance to use as global cache
    """
    global _global_cache
    _global_cache = cache


def clear_global_cache() -> None:
    """Clear the global cache."""
    if _global_cache is not None:
        _global_cache.clear()